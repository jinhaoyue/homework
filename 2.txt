回答内容
“入门”是良好的动机，但是可能作用缓慢。如果你手里或者脑子里有一个项目，那么实践起来你会被目标驱动，而不会像学习模块一样慢慢学习。 另外如果说知识体系里的每一个知识点是图里的点，依赖关系是边的话，那么这个图一定不是一个有向无环图。因为学习A…显示全部
没有被邀请，但是依旧逼格满满地说，这个问题简直就是为我准备的。 第一部分：吐槽+吹牛 （特此说明，本文不针对quant投资的大神） 本人工科本科，毕业后做战略管理咨询，其后做过股票交易员以及VC/PE投资。所呆的机构基本都是行业内的顶尖公司，而个人表现…显示全部
彩蛋： 彩蛋已关闭 骚瑞 为后来的同学解释一下彩蛋怎么回事，顺便对昨晚12点之后收不到彩蛋的同学抱歉（鞠躬），被屏蔽了 彩蛋是如果赞了这条答案会自动收到一条随机的私信，里面是一则短笑话笑话是在某网站上爬下来的，一共几十条随机发送 起因是昨天写完…显示全部
以下是我学python爬虫的打怪升级之路，过程充满艰辛，也充满欢乐，虽然还未打倒大boss，但一路的风景就是最大的乐趣，不是么？希望大家能get到想要的东西！ 多图预警！ 以下奉献一段爬取知乎头像的代码 import requests import urllib import re import ran…显示全部
震惊了，8次鼠标点击，教会你网页数据爬虫 既然零基础，给你讲完python， 讲完正则，讲完网页结构，估计黄花大闺女都嫁人了。 介绍一招，20秒上手爬虫数据，用的工具，Excel不用VBA，甚至都不用公式甚至不用打开网页，查看网络元素也是作为@硅谷密探的补充…显示全部
update: 强烈不推荐搞学术、做量化使用此方法，此方法只适用于商科PPT犬，做一些定性分析时使用。 ----- 我不是搞经济学的，但是最近做实习，要找N多千奇百怪的data，其中有些变态的数据，找来找去都找不到。 但是在某个一霎那，你会突然发现某个report/pap…显示全部
一、券商研究报告下载（收费）彭博 万得 WIND汤森路透 Capital IQ巨灵金融服务平台 同花顺慧博投研资讯 慧博投研资讯-中国最大最专业的投资研究平台（免费）渐飞 渐飞研究报告-全国最大的股票研究报告分享平台 研究报告，投资股票，证投资，行业研究…显示全部
一分钟学会网络数据抓取：从爬虫入门到放弃，鬼知道你都在这期间经历了什么？老司机都忙着反爬虫，没空来跟你闲扯，等你学会新的爬虫程序了，不好意思，老司机们又给你添堵来了，给你添堵就是他们的工作，所以，还是别学了，趁早放弃好吧，还没走…显示全部
2015.11.14 更新神器： 1.下面提到的Quandl网站有一个他们自己的Python库，叫Quandl，可惜也是收费的。 pip install Quandl 2.TuShare -财经数据接口包 国内好心人做的开源财经数据接口（觉得好的可以捐助一下）。这里几乎可以获取到A股的所有信息了，还包…显示全部
第一条：you-get（Releases ・ soimort/you-get ・ GitHub，这里面有各种发布版本）。什么，你不知道？想爬取视频网站的视频和图片分享网站的图片，是不是就得造个轮子写个爬虫？No，你只需要：? pip3 install you-get能干什么呢？我提供几个例子： 1. …显示全部
明确你学爬虫是为了什么。 为了好玩，那就Python3，妥妥的。 话说Python2.7到2020年就停止维护了，并且Python3现在有很多新特性，至于学爬虫需要的第三方库比如Requests、BeautifulSoup也都支持Python3了，这点不用担心。话说现在连Scrapy这个写爬虫的大杀…显示全部
涉及的主题包含编程语言、挖掘工具、设计模式、架构、安全、大数据、分布式系统、推荐系统、搜索、人工智能、脚本等等（不一定是著名站点或者知名博客，重在有可借鉴之处），排名纯按添加时间顺序，持续更新中。。。 Security乌云知识库FreeBuf开放安全研究…显示全部
不会是正常的。如果编程是读个教程就能学会的话，培训班哪还有活路？我才你学习的时候肯定忘了一件事：输出个99乘法表并不是会编程，你得试着去做项目。项目从哪儿来呢？其实无论是知乎还是博客这种社区或者GitHub等专业网站，都已经有非常多的、面向新手的…显示全部
我就是爬虫起家，一年时间机缘巧合进了BAT。又是一帮光点关注感谢不点赞的。感恩大家，上班时间短回头补充哈，勿怪。当你能看懂代码，简单的伪造请求，爬点图片视频搞下实习没问题。当你能单挑分布式做亿以上采集量，各种伪造与绕ip，基本爬虫工作都要你。…显示全部
爬虫(Spider)，反爬虫(Anti-Spider)，反反爬虫(Anti-Anti-Spider)，这之间的斗争恢宏壮阔... Day 1 小莫想要某站上所有的电影，写了标准的爬虫(基于HttpClient库)，不断地遍历某站的电影列表页面，根据 Html 分析电影名字存进自己的数据库。 这个站点的运维…显示全部
一旦走上了编程之路，如果你不把编码问题搞清楚，那么它将像幽灵一般纠缠你整个职业生涯，各种灵异事件会接踵而来，挥之不去。只有充分发挥程序员死磕到底的精神你才有可能彻底摆脱编码问题带来的烦恼，我第一次遇到编码问题是写 JavaWeb 相关的项目，一串…显示全部
有一个利器，能帮你快速爬取你想要的资源…… 有时候，你需要下载电影、音乐的资源，却发现下不下来。 因为你没安装客户端…… 或者是找不到下载按钮在哪 这时候，愤怒的你可能会想要自己写个爬虫来搞定，那么在这里要告诉你，不必重新发明轮子了，有这样一…显示全部
前段时间快要毕业，而我又不想找自己的老本行Java开发了，所以面了很多Python爬虫岗位。因为我在南京上学，所以我一开始只是在南京投了简历，我一共面试了十几家企业，其中只有一家没有给我发offer，其他企业都愿意给到10K的薪资，不要拿南京的薪资水平和北…显示全部
Python入门网络爬虫之精华版Python学习网络爬虫主要分3个大的版块：抓取，分析，存储另外，比较常用的爬虫框架Scrapy，这里最后也详细介绍一下。首先列举一下本人总结的相关文章，这些覆盖了入门网络爬虫需要的基本概念和技巧：宁哥的小站-网络爬虫当我们在…显示全部
要学会使用Python爬取网页信息无外乎以下几点内容： 1、要会Python 2、知道网页信息如何呈现 3、了解网页信息如何产生 4、学会如何提取网页信息第一步Python是工具，所以你必须熟练掌握它，要掌握到什么程度呢？如果你只想写一写简单的爬虫，不要炫技不考虑…显示全部
这几天写了一个爬虫，这是我关于反爬虫的一些总结： 常见的反爬虫和应对方法 - Python Hacker - 知乎专栏 刚开始写爬虫用的是urllib2，后来知道了requests，惊为天人。 刚开始解析网页用的是re，后来知道了BeautifulSoup，解析页面不能再轻松。 再后来看别…显示全部
看了大部分回答不禁叹口气，主要是因为看到很多大牛在回答像“如何入门爬虫”这种问题的时候，一如当年学霸讲解题目，跳步无数，然后留下一句“不就是这样推嘛”，让一众小白菜鸟一脸懵逼。。作为一个0起步（之前连python都不会），目前总算掌握基础，开始…显示全部
用py3写爬虫的话，强力推荐这本书，应该是目前最系统最完善介绍python爬虫的书。可以去图灵社区买电子版。书的内容很新也很系统，从beautifulSoup，requests到ajax，图像识别，单元测试。比起绝大多数blog零散的教程要好的多，看完书后就可以去做些实战项目…显示全部
爬虫无非分为这几块：分析目标、下载页面、解析页面、存储内容，其中下载页面不提。 1. 分析目标所谓分析就是首先你要知道你需要抓取的数据来自哪里？怎么来？普通的网站一个简单的POST或者GET请求，不加密不反爬，几行代码就能模拟出来，这是最基本的，进…显示全部
我觉得爬虫（不是谷歌那种大型的）是世界上最无聊最没有技术含量最累的编程活动。 理由： 1，你需要费三分之一的时间在解析网页上，分析网页结构，写 XPath 。有的时候，你需要的东西混在一段 JavaScript 里，这就很脏了，你需要使用字符串截取来取出那段 J…显示全部
凡是问一件事儿道德上有没有问题，基本上都可以分成两个问法：复杂问法和简单问法。 复杂问法就是总会被一众人等说成“你不能简单的问一件事道德上有没有问题，情况是很复杂的”那种问法。比如，你问，杀狗是道德的吗？然后立刻就会有各种人跳出来给你区分…显示全部
说说我的经历吧 我最早是爬虾米，想看看虾米听的比较多的歌是哪些，就爬了虾米全站的歌曲播放数，做了个统计 Python爬虫学习记录（1）――Xiami全站播放数 统计过豆瓣动漫的评分分布 豆瓣2100部动漫页面的网页源码(包括评分，导演，类型，简介等信息，附抓…显示全部
谢邀。 这个问题不算小。不过题主也不算是伸手党，也就不用给出所有一步一步的代码了。 我给出大概的介绍、思路和一些关键部分的代码，题主再自行尝试怎么实现你的项目吧。 把一些坑放在前面…提醒你注意并自行解决。 编码问题 Python2 Python3 版本不同 首…显示全部
注意：如下内容我来自之前的一篇博客，与题主遇到的实际问题并不完全匹配，但是目的依然是为了解决 Python2 中的编码问题。 -------------- 在 Python 尤其是 Python2 中，编码问题是困扰开发者尤其初学者的一大问题。什么 Unicode/UTF-8/str，又是 decode/…显示全部
自问自答的问题，缘起是因为数据挖掘入行不久，一直上拉勾网看各种公司的招聘JD，人工看一方面是时间很消耗，更严重的是抓不住重点，最近刚好入手python爬虫，试图简化这部分工作。另一方面学习爬虫之后，发现自己整天上网手动翻网页找信息这个动作很low，…显示全部
无论是什么设备，第一次访问该站，都会弹出一个521的错误状态码，与此同时还会返回一个Cookie。 浏览器接受到状态码与Cookie，会再次进行一次请求，因为接收到了Set-Cookie，所以第二次的Request Headers会附上之前接收到的Cookie。 这样的请求才是成功的。…显示全部
直接看爬虫框架有时会很吃力，建议从简单的程序一步步入手，看到脚本之家有一个系列讲述的一个Java爬虫程序的设计，在此拿过来大家共同学习。 http://www.jb51.net/article/57193.htm首先以百度首页为例通过http get的方式获取百度首页的内容 import java.i…显示全部
转一篇崔广宇老师16年的分享，尽管整个分享过程中并没有太多奇技淫巧，但是对于反爬虫的“道”，阐述及其清晰。因为爬虫和反爬虫的斗智斗勇，学道显然要比学术更好。原文在这里：关于反爬虫，看这一篇就够了你被爬虫侵扰过么？当你看到“爬虫”两个字的时候…显示全部
1.nutch 地址：apache/nutch ・ GitHub apache下的开源爬虫程序，功能丰富，文档完整。有数据抓取解析以及存储的模块。 2.Heritrix 地址：internetarchive/heritrix3 ・ GitHub 很早就有了，经历过很多次更新，使用的人比较多，功能齐全，文档完整，网上的…显示全部
说一个关于网易云音乐的把对付一些小白是可以的，因为他们主体页面是异步加载嵌套在iframe里面的，并且 src="about:blank"  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~割 看了@ peng yang 兄弟的问题，说一下去哪儿的该怎么爬选择去哪儿的m站去哪儿网--聪明你…显示全部
python在网页爬虫、数据挖掘、机器学习和自然语言处理领域的应用情况如何？ Python的快速迭代能力让它收到青睐。按照楼主的问题一个个回答，结合我有限的经验： 1）爬虫Scrapy，简单易用。用rq-queue结合的话很容易构造一个分布式的爬虫。我曾经这样爬下了…显示全部
=============2016.10.21 更新 ============ 斗鱼已经开放了第三方协议和代码，大家可以搜索看看；我的代码也更新了下，为了性能，很多地方用的hard code，大家随意看看就好，千万别学我！(用golang也写了一份) github路径：GitHub - fishioon/douyu: Get d…显示全部
其实一开始就有同学邀请我回答，昨天 @Manjusaka 提到了我，本来我就想来点点赞就算了，结果在床上刷了会知乎，又刷出来3篇爬虫文章和一篇某本爬虫新书的推广内容，心情不好，来说说我的分析和理解，想直接看结论的跳到第四点。1. 为什么python分享中爬虫这…显示全部
//好多人说现在关注就有提醒的……呃，题主提问的时候显然没那个功能。我写这个只是自己在学习爬虫过程中的练习。我不炒股也不上雪球…… //好多赞。容我安利一篇自己的回答如何入门 Python 爬虫？ - 段晓晨的回答 边做边调边写~ #start coding 首先要知道…显示全部
context_re = r'<div.*?class=\"cont f14\".*?id=\"(.*?)\">(.*?)<br />' 你准备的这个正则表达式啊，truncated！断在了<br/>这里，所以只能爬第一段。 爬取新浪军事论坛需要做三件事： 一、 上CSDN汪海老师的专栏，http://blog.csdn.net/column/details/why-bug.html，学习一个。 二、 …显示全部
听我说：一开始千万不要用框架，不管是Scrapy还是什么其他框架。 你就老实的用Requests、用BeautifulSoup4、用lxml、用正则表达式、用多线程、用异步、用分布式如果你需要的话、用代理设计代理策略、自己设计抓取策略。 然后突然有一天你上手了Scrapy框架，…显示全部
在我博客连载的我写的爬虫系列教程，欢迎与我联系探讨～～～～ 希望能帮到你 爬虫教程（1）基础入门 爬虫教程（2）性能进阶 知乎用户信息爬虫（规模化爬取） 用scrapy爬取豆瓣电影新片榜 用scrapy对豆瓣top250页面爬取（多页面爬取） 用scrapy自动爬取下载…显示全部
嗨~我来答题了~ 虽然题主已经搞定了问题…… 提问后一周已经搞定了，用了excel power query+yahoo finance api 等这周忙完毕业设计回来更新问题…还是非常感谢～！ 就当练手了~问题的解决办法有很多。利用现有的api挺方便。不过我还是按照题主原来的思路笨…显示全部
前段时间在研究sina和twitter的数据，很久之前写得一个模拟登陆的脚本没用了，于是朋友给我发了一份有用的模拟登陆脚本，亲测有效。我改造了一下做的爬取微博评论的脚本效果如下：下面是代码，结合以下代码理解一下，希望对你有用。# -*- coding: utf-8 -*-…显示全部
Python入门网络爬虫之精华版Python学习网络爬虫主要分3个大的版块：抓取，分析，存储另外，比较常用的爬虫框架Scrapy，这里最后也详细介绍一下。首先列举一下本人总结的相关文章，这些覆盖了入门网络爬虫需要的基本概念和技巧：宁哥的小站-网络爬虫当我们在…显示全部
大部分爬虫其实都是写体力活，这些入门足矣（由于时间原因，所以不保证文中的方法现在依然可用，也不保证其准确性，仅供参考~）： 爬虫必备――requests 01. 准备 02. 简单的尝试 番外篇. 搭建称手的Python开发环境 03. 豆瓣电影TOP250 04. 另一种抓取方式 …显示全部
Diycode 开设了开发者酷站，目前收录了将近70个酷站&开发者博客：开发者酷站 - DiyCode Hacker Blogs：对技术更加深刻的理解，文字总给人以深刻的启迪。 云风的 BLOG 吴云洋（Cloud），出生于1979年2月5日，原网易游戏核心成员、杭州研究中心总监。 吴云洋…显示全部
按照题主的描述有仔细看了下北邮人的论坛 ，此网站的所有网页内容请求的 url 应该都是可以通过抓包分析出来的。比如说我们看下其中的一个优秀版主评选板块：北邮人论坛-北邮人的温馨家园 ，使用Chrome的F12或者其他任何你比较熟悉的工具抓包，其他板块同理…显示全部
遇到过的：1、根据一定行为特征，封IP或者弹验证码。2、蜜罐，确认是爬虫之后，返回虚假数据。这套玩法，厚颜无耻得说，在线下我算是先行者。当年汽车大V@王洪浩 和我一个宿舍，老是偷偷把公用电脑上我的作业拷走去交差（那个时候的学生穷呀，只能一个宿舍…显示全部
教你一个爬虫小技巧：所有社交网站爬虫，优先选择爬移动版，比如：http://m.weibo.com显示全部
一时兴起写了篇回复，能给大家解决一点小问题，十分高兴。但偶尔被误认为是大牛，诚惶诚恐。 只是当初一点小爱好而已，大牛实在是不敢当。 另外要对私信给我请求帮助的朋友说声抱歉，我已经离开技术岗位很长时间了，手头上早已没有了编程的环境，加上目前工…显示全部
我是来吐槽最高票的@Leaf Mohanson 虽然学习的确应该追求本质，但是如果一个学习过程太过冗长又没有实质性进展，很容易让人失去继续学习下去的动力。 比如说，验证码破解（一般不谈黑产链的活，下不为例），居然推荐了pandas和numpy。 如果题主没有相关的基…显示全部
完整代码 + 详细解释见： [Python爬虫] 「暴力」破解猫眼电影票房数据的反爬虫机制 - 集智 打开浏览器控制台可以发现，票房数据其实是加密过的生僻unicode编码，而且每次访问获得的unicode是随机生成的。也就是说，明文攻击只对单次访问有效。 而前端的阅读…显示全部
虽然这是一个很久以前的问题，题主似乎也已经解决的这个问题。但是看到好多答案的办法有点太重了，这里分享一个效率更优、资源占用更低的方法。由于题主并没有指明需要什么，这里的示例取首页所有帖子的链接和标题。 首先请一定记住，浏览器环境对内存和CPU…显示全部
又是你这个只点感谢不点赞的坏人…… 话说你为什么要这么心急地学呢，基础不扎实啊，太冒进了，很显然没有清晰的思路… 首先编程要有默认的编码，也就是在文件的一开始加上 # -*- coding: utf-8 -*-其次，你读取到网页的时候，需要查看网页的编码，虽然现在…显示全部
写了个关于模拟登录常见网站的小项目，GitHub - xchaoinfo/fuck-login: 模拟登录一些知名的网站，为了方便爬取需要登录的网站其中包括知乎 百度 新浪微博 126 邮箱 web微信等，考虑了 Py2 Py3 版本兼容 以及验证码的问题，欢迎大家来围观 pull request既然…显示全部
（用上海话说）能做的事情不要太多喏。 核心的一些知识点：数据抓取（ETL），NLP（分词、情感分析、语义分析等等）以及SNA（Social Network Analysis，网络理论中的各种指标）。 1、关键词统计及分词 利用知乎的热门回答（沧海横流，看行业起伏（2015年） -…显示全部
我只说两点： 任何网站有权拒绝其他网站抓取自己的内容，别拿互联网开放来给自己的商业目的打掩护；淘宝也屏蔽了百度的爬虫，以阻止百度的抓取，京东的做法跟淘宝并无二致。 比较令人反感的，是一淘对刘强东的回应：“新商业文明和诚信，您可以不信但我们信…显示全部
不出意外被举报了「不宜公开讨论的政治内容」，还好已经提前把资料存到了Github博客上。 欢迎点进来学习一个：Python 3 多线程下载百度图片搜索结果显示全部
1、Gecco github地址：xtuhcy/gecco Gecco是一款用java语言开发的轻量化的易用的网络爬虫。整合了jsoup、httpclient、fastjson、spring、htmlunit、redission等框架，只需要配置一些jquery风格的选择器就能很快的写出一个爬虫。Gecco框架有优秀的可扩展性，…显示全部
爬虫自动换User-Agent在代码实现上只需要一句就够了，并不是什么高难度的技术活。爬虫为什么要换不同的User-Agent呢，无非就是为了模拟浏览器，让服务器不容易识别出自己是爬虫。 对于爬虫，其实上上策是爬网站之前先去读一下网站的『robots.txt』文件，看…显示全部
Update 20160609 : 更新Python客户端,修复由于斗鱼网页版面修改带来的小问题,直接开启海量弹幕模式(请大家不要问我为什么端午节这一天为什么闲着没事更新代码,这个真的和情人节是同一个原因). GitHub - twocucao/danmu.fm: douyutv danmu 斗鱼TV 弹幕助手 U…显示全部
说下自己的爬评论分析过程吧（其实是为了爬评论才研究的，但是评论数和评论的数据在一个json中，虽然有点文不对题，但还是希望能对题主有帮助） 首先因为我们点击下一页看后面的评论时是只有评论刷新的，所以这里获得评论数据是发xhr（XMLHTTPRequest），那…显示全部
题主快点自首吧，写过高频交易系统比写脚本抢月饼严重多了，你写的时候考虑过普通股民的利益没有。你这分分钟千万的上下，比几盒月饼严重多了，这都不是阿里价值观的问题了，这是严重违法啊。 赶快自首吧，回头是岸。显示全部
不邀自答. 零基本或者更多没有太多编写完整项目经验的同学, 在初步了解了Python基本语法之后, 可以通过以下方法深入学习: 写爬虫, 学习Scrapy框架(难度两颗星)建网站或者建立博客, 学习Django框架, 熟悉后转flask框架(难度三颗星)学习numpy等包, 用python做…显示全部
本来只是看题主也努力研究了，才写的小攻略。意外的受欢迎赞还不少呢，有兴趣的童鞋移步文末，更多精彩。你这也花了不少心思，我提供一个小攻略吧，，写此类post请求的时候，不要自己去构造，搜一下postman，，你会发现这是多么轻松。。尤其是爬虫写多了以…显示全部
谢邀。 最近快期末了，很多邀我的问题都忽略了，简单看了一下题主的主页，似乎是信安专业的。我也是信安的，深有同病相怜之感，所以来答。 信安作为一个计算机、通信、数学的交叉学科，开设课程是大都是理工科的基础课程和一些简单的编程，简单涉及密码学和…显示全部
先说结论：在爬虫与反爬虫的对弈中，爬虫一定会胜利。换言之，只要人类能够正常访问的网页，爬虫在具备同等资源的情况下就一定可以抓取到。 robots.txt 只是约定，爬虫遵守或者不遵守完全在于爬虫作者的意愿。举个例子，公交车上贴着「请为老弱病残孕让座」…显示全部
知乎上大部分所谓的“爬虫”程序都是 Hello World 级别的 轮循 HTTP 请求小程序，说好听点叫定向爬虫。 发起 HTTP 是最基本也是不重要的环节，真实的能称得上 “爬虫” 的程序架构跟上面我指的那些 Hello World 有天壤之别。数据结构、多线程、算法等都是很…显示全部
这里以个人专栏做一下示例获取专栏所有文章信息和专栏相关信息（包含代码） 第一步：专栏地址：https://zhuanlan.zhihu.com/passer 后面的passer是作为专栏的唯一标识。通过任何一工具抓包，得到url： https://zhuanlan.zhihu.com/api/columns/passer 返回的json数据。 抓包出来的数据展示为（用浏…显示全部
先问是不是，再问为什么。答案是他们的 robots.txt 都写错了。必应是遵守了 The Robots Exclusion Protocol 的。 遵守或者不遵守本来就是无所谓的。这又不是什么行业标准。这协议连道德标准都算不上。 先说 robots.txt 该怎么写 以下引用自 The Web Robots …显示全部
那些给你安利什么gevent/xpath什么的人，一定没有写过爬千万级或者亿级以上数据量的爬虫。 按照你的需求，你有1500万条数据需要抓取，这1500万条数据分布在100万页网页中。那么，从以下几个方面给题主一点建议。 首先，存储的问题，爬虫的数据，尤其到了一…显示全部
1.反爬虫其实非常非常简单，反反爬虫才麻烦。（你没看错，是“反反爬虫”，我没打错字）。 2.反爬虫的关键是抓住爬虫的特点：爬虫是机器，不是人： 特点1：非人，所以爬虫访问速度快，访问次数多。 特点2：非人，所以人做不到的事情爬虫能做到。 特点3：非…显示全部
MongoDB作为非关系型数据库，其主要的优势在于schema-less。由于爬虫数据一般来说比较“脏”，不会包含爬取数据的所有field，这对于不需要严格定义schema的MongoDB再合适不过。而MongoDB内置的sharding分布式系统也保证了它的可扩展性。MongoDB的aggregatio…显示全部
反对 @Kenneth ，他见过的爬虫太少了 首先取决于目的 如果是一个站点，单一目的，用习惯的语言写吧，学别的语言用的时间都够重构两遍的了。 如果是有100左右的站点，做个框架，把你的爬虫管理起来，比起怎么写更重要。 ok，以上两个都是 “手动” 写模板的…显示全部
Robots协议防君子容易防小人 难。  robots.txt（统一小写）是一种存放于网站根目录下的ASCII编码的文本文件，它通常告诉网络搜索引擎的漫游器（又称网络蜘蛛），此网站中的哪些内容是不应被搜索引擎的漫游器获取的，哪些是可以被漫游器获取的。因为一些系统…显示全部
作者：唐平 链接：遭遇CryptoWall 4.0勒索病毒应该如何解决？ - 唐平的回答 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 你好，作为一个2013年开始追踪勒索病毒的老师傅来讲，你中的病毒我确定是Cryptowall. 以下内…显示全部
当你说“百度”这个词时 你就和道德划清了界限。显示全部
开发网络爬虫应该选择Nutch、Crawler4j、WebMagic、scrapy、WebCollector还是其他的？这里按照我的经验随便扯淡一下：上面说的爬虫，基本可以分3类： 1.分布式爬虫：Nutch 2.JAVA单机爬虫：Crawler4j、WebMagic、WebCollector 3. 非JAVA单机爬虫：scrapy…显示全部
广告显示全部
悲剧，反正我是没机会了。 爬了这七年的X-ART，调用百度翻译API，把简介翻译成了中文。 后面是入库，智能分析，智能推荐，模特预测。 不过发现没什么意思了。 补充点想法。 X这几年一直在进步，高清越来越多，体积也越来越大，从评分就能看出，质量在提高。…显示全部
年轻人总是这么幼稚，图样图森破。你能用nodejs写爬虫，觉得nodejs比python好，你去写就成了。这跟取代python有什么关系，别老想着一统天下。世面见多了就知道世界复杂的狠，别老想取代这个取代那个，老老实实搬砖，踏踏实实做人。显示全部
1 第一次先请求某个网页，抓取到本地，假设文件名为 a.html。这时文件系统有个文件的修改时间。 2 第二次访问网页，如果发现本地已经有了 a.html，则向服务器发送一个 If-Modified-Since 的请求（http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html）。 把 a.html 的修改时间写到请求里…显示全部
真的不要纠结2还是3，对于爬虫来讲，感觉不到区别，这些都不是事儿，除了编码和print。 而且requests和bs4都支持吧（待我确定下）。 那什么是事儿呢？ 1 限制ip 用requests代理，买代理，或者网上免费代理 2 伪装成浏览器 requests切换user agent 3 先登录…显示全部
谢邀，不笑不笑。术业有专攻而已。 网络爬虫要解决的问题有如下几点： 1，页面下载。页面上往往有我们需要的信息，如链接，图片，点评等等。前提是能把他们下载下来。 2，链接提取。初始化为一些链接，然后不断抓取的新的链接。 3，URL管理。防止重复和陷入…显示全部
Python在这方面的package非常齐全： 网页爬虫: scrapy（不太清楚） 数据挖掘: numpy, scipy, matplotlib, pandas（头三个是业界标准，第四个模拟R） 机器学习: scikit-learn, libsvm（极好） 自然语言处理: nltk（极好）显示全部
防爬虫，简单来说，就是在尽量减少对正常用户的干扰的情况下尽可能的加大爬虫的成本。 而反防爬虫，或者说制造隐蔽性强的爬虫，就是尽量模拟正常用户的行为。 这两者是动态博弈的。大公司基本都有防爬的项目，以保护数据安全。 你去试试爬几个大网站就知道…显示全部
#!/usr/bin/env python# -*- coding: utf-8 -*-import requestsfrom bs4 import BeautifulSoupimport timeimport randomimport sysdef get_post_data(url, headers): # 访问一次网页,获取post需要的信息 data = { 'TAB_QuerySubmitSortData': '…显示全部
Python3爬虫视频学习教程 | 静觅大纲是这个样子的：一、环境篇Python3+Pip环境配置MongoDB环境配置Redis环境配置MySQL环境配置Python多版本共存配置Python爬虫常用库的安装二、基础篇爬虫基本原理Urllib库基本使用Requests库基本使用正则表达式基础Beautifu…显示全部
再最新：评论区再爆料疑似使用ip代理暴力爬的路很可能鲁班门前弄斧了.....具体看楼下详情某不知名热心评论主; lanlan........ 几个回合下来，大家互相打死了各自的脸round 1：我早年在阿里“友商”的一家小公司工作过，爬虫被教做人后来我听阿里的某次技术…显示全部
这个问题不算麻烦，中午休息的时间简单调试了一下。我把步骤、代码和一个月的数据 demo 贴上，希望题主有一定的基础，然后能自己把需要的所有数据跑出来。 最后给出的完整代码修改起止日期以后应该是能直接使用的。我中间有短暂地无法访问这个网站，所以在…显示全部
现在关注一个组合，就会有持仓变动的提示了。不过我觉得这事情挺有意思的。比如可以把很多持仓的数据都抓下来，做一些综合的分析，看看现在网站上被持有最多的股票是哪一支，某一天被调入最多的又是哪一支之类。 于是我决定来抓抓看，顺便借此说说我通常用…显示全部
最近我们在做类似的工作，一方面接单独的定制需求，另一方面做一个无需编程的智能云爬虫网站。可以来我们这里看看：造数 - 最好用的云爬虫工具我们精心制作了视频：造数云爬虫使用教程-------------------------------------------------------------- 因为…显示全部
金融从业的范围其实听广泛的，有的岗位并不需要从业者熟悉Python或者其他编程语言，比如在证券、期货或者某些资产管理单位从事风控工作，基本上现有的IT系统功能已经很完善；从业者只是这些系统的用户，有新需求时通常要求系统的开发者来设计和开发，很少需…显示全部
我来讲一个特别的。当年在MSRA的时候写爬虫，就不需要访问网络。我先以项目的原因跟Bing要资源，当然同时有它的集群和他的数据。爬虫就是用Scope写的，所谓的Scope长得很像SQL，然后在所有有表达式的地方都换成了C#代码。所以基本都写成map-reduce那样子的…显示全部
看了一下京东的商品评论接口已经变化了，变成了下面这个样子。 只需要模拟几个参数就可以了。 productId：商品的ID，如果没有商品ID可以在商品静态页面抓取到。 sortType：评论按照什么排序：推荐排序和时间排序，默认5 page：当前是第几页评论，从0开始递…显示全部
写爬虫对绝大多数人根本没有意义，学Python还是从基础入门好，理解函数、高阶函数、生成器、类对初学者更重要：Python教程显示全部
如果把Python类比Linux，那么Anaconda就是centos、ubuntu之类的 Anaconda 是一个可用于科学计算的 Python 发行版，支持 Linux、Mac、Windows系统，内置了常用的科学计算包。它解决了官方 Python 的两大痛点。第一：提供了包管理功能，Windows 平台安装第三…显示全部
大兄弟，你抓包没抓对啊！ 这两个url虽然长得像，但仅仅是长得像噢！ http://music.163.com/playlist?id=317113395http://music.163.com/#/playlist?id=317113395 真正有歌曲信息的是上面那个url，在上图中很明显的！既然知道了那我们就直接来抓取吧！只…显示全部
微博没有抓过，知乎倒是抓过。最近仍然在抓（微博受了某位大大的邀请最近也在准备抓，等抓了再来补充一下回答）。 那么先说一下知乎吧，一下根据题主的问题描述回答。 问题一：1.爬取>10k的用户以及他们的互粉信息 如果你想通过自己的努力去得到这些信息，…显示全部
java爬虫 最好用不过 code4craft/webmagic ・ GitHub ， 谁用谁知道。 国人 黄亿华 先生的良心大作 文档在这里 http://webmagic.io/docs/zh/显示全部
这个并没有其它答案中说的那么可怕。 一般来说，由Javascript生成，基本会有Ajax请求。那就直接打开浏览器，然后F12.找到Network， 然后悬着XHR就好了。例如知乎： 嗯。看到了吧，里面有请求的链接，你直接请求这个地址就好了。 当然等多的时候，这个链接可…显示全部
spider163已更新v2.0！ 公众号文章推介：非重磅 | 网易云音乐爬虫Spider163更新v2.0 项目源码及简介：Chengyumeng/spider163―――――――――――――――――――――――――――――――――泻药，以我抓取了307835首网易云音乐的歌单歌曲的经验，讲一…显示全部
Fiddler可以干的东西太多了，甚至在早期曾经救了我的知乎账号 1.动态Web调试，可以抓取http&分析之，这个不说了，别的很多抓包软件也可以实现 2.配合Proxifier可以抓取&代理原本不支持http代理但使用http协议通信的应用 3.中间人攻击，因为fiddler本质是个h…显示全部
如果为了简单，建议去看requests，如果想深入，当然从写框架开始，以下内容引用自我的专栏文章。 ---------------------------------------------- 虽然不是专业的爬虫工程师，但作为一个Pythoner，一直对爬虫情有独钟。 Python有很多爬虫框架，比如Scrapy…显示全部
那些动用浏览器内核（PhantomJS、Selenium等）的方案都太重了， 上面链接中的博客通过抓取拉勾网 Ajax 请求的数据示例讲解了如何抓取前端渲染的网页，也就是题主说的 「网页内容是由 JavaScript 生成的」，摘抄如下： 链接：http://xlzd.me/2015/12/19/pyth…显示全部
粘贴我之前的回答。我一直用Java写的爬虫 爬虫的入门是很快的。 了解HTTP协议，了解几个API，就可以直接上手写代码了。 推荐：《自己动手写网络爬虫》一书，里面详细讲解了爬虫的基本原理和实现，尽管API有些过时了，但是思路是很棒的。 爬虫的学习分为下面…显示全部
人家都提供API了你就不要爬了，对网站不友好自己又要费劲解析。 获取图书信息GET https://api.douban.com/v2/book/:id 返回图书信息，返回status=200 对于授权用户，返回数据中会带有该用户对该图书的收藏信息： { … (图书信息的其他部分) "current_use…显示全部
又到了和炫酷的造数一起研究爬虫工具的时候了！一、什么是Python爬虫框架简单来说，Python的爬虫框架就是一些爬虫项目的半成品。比如我们可以将一些常见爬虫功能的实现代码写好，然后留下一些接口，在做不同的爬虫项目时，我们只需要根据实际情况，手写少量…显示全部
自问自答一下 可以用一个很牛逼的包，叫做selenium（官方文档Selenium with Python），简单来说就是模拟人对浏览器的动作，可以用代码打开你的浏览器然后像人一样操作实现浏览器的自动化（打开网页、输入文字、提交表单等），安装等详细介绍在官方文档中有…显示全部
其实单纯地做一个能用的爬虫十分简单，只需要理解最基本的知识就行啦，任何语言都能做，只是难易问题。 网站运行在web服务器上面，比如tomcat，apache这种，这些web服务器本质上就是一个做得很吊的socket服务器。 任何看过一点socket入门教程写过小例子的人…显示全部
爬虫代理IP池在公司做分布式深网爬虫，搭建了一套稳定的代理池服务，为上千个爬虫提供有效的代理，保证各个爬虫拿到的都是对应网站有效的代理IP，从而保证爬虫快速稳定的运行，当然在公司做的东西不能开源出来。不过呢，闲暇时间手痒，所以就想利用一些免费…显示全部
知乎专栏API来一发？ 举俩栗子： URI: http://zhuanlan.zhihu.com/api/columns/jixin GET/HTTP 1.1访问上面的URI，浏览器地址栏里直接粘贴也行，得到的返回JSON数据就包含了专栏关注数。 不管AngularJS还是其它架构，都是服务端的东西，再天花乱坠的服务端…显示全部
现如今，互联网上各种各样的HTTP代理IP软件层出不穷，到了选择的时候纠结症就开始犯了，开个玩笑，这种纠结不亚于去想中午吃什么。因为我自己运营一个网站，所以需要网站优化。这里就算是给大家一个小福利吧，目前用的是太阳HTTP代理，感觉还可以，很多朋友…显示全部
当然是抓摩拜的微信小程序最方便了，手机挂上代理，打开摩拜微信小程序，过滤条件设置为 http://mobike.com，抓到下图的包：观察请求，猜测 /mobaike-api/rent/nearByBikesInfo 是获取附近单车的包，点开一个详情：复制 「response body」中的内容，随便…显示全部
谢邀，这几天实在太忙，没时间写技术答，偷懒将之前写的一篇爬虫入门贴过来吧。所贴代码皆为当时运行通过的，应该是最入门的历程了，是不是干货题主自行判断 ---------------------------- 作者：洪宸 链接：Python 爬虫进阶？ - 知乎用户的回答 来源：知乎…显示全部
之前回答过源于java爬虫的问题，转过来。 推荐如下的java开源爬虫或抓取框架 1.webmagic 【猪猪-后端】WebMagic框架搭建的爬虫，根据自定义规则，直接抓取，使用灵活，Demo部署即可查看。 官站：WebMagic 2.jsoup java网络爬虫jsoup和commons-httpclient使…显示全部
几个微博爬虫开源项目： SinaSpider- 基于scrapy和redis的分布式微博爬虫。SinaSpider主要爬取新浪微博的个人信息、微博数据、关注和粉丝。数据库设置Information、Tweets、Follows、Fans四张表。爬虫框架使用Scrapy，使用scrapy_redis和Redis实现分布…显示全部
你都没想明白为什么要分布式 ======================== 我还是认真答一下吧，爬虫这种东西在大批量抓去时主要有下面几个量变引发质变的挑战： 1. 出口IP数量，主要是考虑防止被封禁，带宽反而不是大问题，这个问题可以通过搭建NAT出口集群，或者单机多IP的…显示全部
职业采集。 爬虫写代码中最耗时的是反爬虫的问题。。开始写代码之前先检查。。你可以先用scrapy的shell去请求你这个四十个网站的数据页面，如果都能拿，那说明反爬虫很一般，就直接scrapy来写，因为这样写的代码复用率很高，一般改正则和队列即可。 但是在…显示全部
Node.js当然适合做爬虫，当然Python同样也适合。你所遭遇的问题充其量只是个程序逻辑问题，而不是语言问题。Nodejs做爬虫最大的优势大概在于更容易接入诸如phantomjs/casperjs来搞一些更自动化的针对动态加载内容的爬取（当然Python也行，只不过没那么原生…显示全部
第一条建议是：去租个服务器啊，现在服务器又不贵，如果你是学生优惠多多，你要抓那么多数据的话肯定是要放在服务器上跑，放电脑上跑多累人啊。 然后再来谈一谈说如果你真的不愿意租服务器的话。 1、如果说要抓取的url都是有规律的，并且当你因为特殊情况停…显示全部
去年开始研究做爬虫，搞了一套分布式的爬虫系统，主要目标是帮别人做数据采集。后来看到黄焖鸡米饭是怎么火起来的？ - 何明科的回答，进而关注了《数据冰山》，发现里面的大数据分析的文章都相当有意思，图表也一个比一个专业。我当时的表情大约是这样的：…显示全部
自学吧！要注意应用。 显示全部
登录很简单，其实上面很多答案的很多内容都是可以去掉的。简化到最后奉上以下代码。(是手机号码登录的，想要邮箱的话改一下url和参数就可以了) #encoding=utf8import cookielibimport urllib2import urlliburl_start = r'https://www.zhihu.com/topic/…显示全部
不谈道德（谁理亏明摆着），只谈法律。 之前大众点评网曾状告爱帮网不正当竞争，北京市一中院2011年7月终审判决爱帮网构成不正当竞争：“爱帮网对大众点评网的点评内容使用，已超过了适当引用的合理限度，事实上造成爱帮网向网络用户提供的涉案点评内容对大…显示全部
没用的，就算你解决了503的问题，你很快也会发现amazon返回给你的价格都是错的。显示全部
先检查text是什么类型 如果type(text) is bytes，那么text.decode('unicode_escape') 如果type(text) is str，那么text.encode('latin-1').decode('unicode_escape')显示全部
不会用openssl，于是想自己实现个类似的。 这是学不会开车于是想自己造一辆么 ======= 我一直觉得想学东西起码得有点钻研的精神和研究的耐性，碰到困难了想办法自己解决，这才是学习。 如果这个问题的题目是“编译openssl库后发现无法使用，xxxx现象，请大…显示全部
不请自来，知乎首答，同为大四毕业狗 之前帮老师爬过这个信息，从1995年-2015年有170多万条，算了下时间需要40多个小时才能爬完。我爬到2000年就没有继续爬了。当时写代码的时候刚学爬虫，不懂原理，发现这个网页点击下一页以及改变日期后，网址是不会变的…显示全部
看完《孙子兵法》就会指挥军队了么？ 上完课，你不是什么也不懂啊，你只是不会将所学到的知识应用到现实世界中而已。 看书只能获得知识，实践才能掌握技能。 至于你觉得无从下手，是因为你潜意识根本就懒得下手，而总想一次性将问题解决……美其名曰寻找最…显示全部
有关这个问题。前一阵开发过。 ～～～2015，7，21补充代码地址。 应评论里几个知乎网友要求。我把代码放github了。先说一下。这些代码是自己思考和参考了nodejs以及网上很多资料的。也感谢他们。此外代码还一直没时间完善完毕。需要用的人还需要自己努力去…显示全部
这两天刚好做了一个抓取用户的关注着和追随者的的爬虫在抓数据，使用的是Python。这里给你一段python的代码，你可以对着代码看一下你的代码问题。 403应该就是请求的时候一些数据发错了，下面的代码中涉及到一个打开的文本，文本中的内容是用户的id，文本里…显示全部
简单爬虫不难，无非发起http访问，取得网页的源代码文本，从源代码文本中抽取信息。首先要自己会写代码。学习爬虫可以从下面一些知识点入手学习。 1、http相关知识。 2、浏览器拦截、抓包。 3、python2 中编码知识，python3 中bytes 和str类型转换。 4、抓…显示全部
用 requests 库发请求的爬虫相比用 scrapy 框架写的爬虫，就好比组装机之于品牌机。 对于大部分普通 PC 用户，品牌机是一个不错的选择，因为品牌机提供了整套的打包方案，你不需要知道很详细的细节就可以抱回家一台各方面表现均衡的电脑，不过，这台电脑仅…显示全部
谢邀！ 做爬虫有没有大的发展？答案是肯定的，但是不一定是在爬虫工程师这个方向。 先说说你要做好爬虫要了解哪些知识？ 1.HTTP 协议： 开发爬虫 HTTP 协议是首先要了解的，你与 HTTP Server 通讯是基于这个协议，各种请求方式、规则、状态。只有基本了解了…显示全部
添加后门啊，他们使用app的时候发点东西到服务器，自己统计。不过你只能统计Android的情况，因为iOS和WP的使用习惯都很安全，干不了这种事情。显示全部
你的问题描述是： 比如说，我写一个 headless browser，每次随机一个时间段，然后随机点击某些章节，相当于可以一直给文章提高点击率。 那么问题来了，那些网站有监管机制嘛（比如一个IP点击上限之类的？）？ 尤其是像晋江这种，我感觉连 UI/UX design 都没…显示全部
关于用 Python 自娱自乐，之前回答过好多了，有兴趣的可以翻翻我以前的答案，就不再赘述了，这里说一个最近几天写的小程序吧~~~ 之前看到一个答主在某问题下分享了三百多张带有 “我爱你” 字幕的电影截图（抱歉当时忘了记下回答链接，所以这里没办法贴出原…显示全部
前一阵金融系的同学委托我写个Python爬虫从某小额信贷网站爬订单细节（公开的），共20万条，用了多线程还是嫌慢。比较奇怪的是这么频繁的访问它不封我IP…最后一个搞安全的同学帮我直接拖库了。显示全部
慢慢来。 1. 添加user-agent，header。避免一开始就被屏蔽掉。推荐用urllib2，requests(最近才用这个，发现很好用) 2. 编码用utf-8，本地存储的时候用codes.open 来保存中文字符 3. lxml解析的速度要比beautifulsoup快的多 4. 如果beautiful和lxml都不能抓…显示全部
1、入门首先用java搞爬虫，当然得学好java的基础咯，什么循环/顺序/条件这是基础中的基础，还有多线程，IO（同步/异步/阻塞/非阻塞）等等都不能落下。然后是爬虫的基础，HttpClient（现在应该叫HttpComponents了）是java里最常用的下载类库，最好要熟悉，可…显示全部
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~百度POI代码修正 定量城市规划研究Series01 ・ Part I：用Python采集百度POI数据 更新，推荐以上微信公众号中的实现代码，亲测可以实现。它是把地图按矩形区域划分，按矩形区域获取。应该可以突破400限制。…显示全部
你代码写的问题，应该用HttpClient+Dataflow，异步发送请求，我通常都要限制并发量避免网站被我爬死。 不过，我觉得你的那个结果出现的原因是莫名其妙的Sleep逻辑导致的。 通常来说语言只能决定性能的上限，但是程序员决定了性能的下限。显示全部
我没在超大型公司待过，但是作为一个python爱好者和使用者，简单说一下自己的想法。 （1）python是一门语言，语言只是工具。比语言更重要的是编程思想。同样的功能，老手可能需要10行python代码，新手可能需要50行。想要练习编程思想，唯有多读多写多练。 …显示全部
import requestsfor i, j in enumerate(imgUrlList): with open('E:/{0}.jpg'.format(i), 'wb') as file: file.write(requests.get(j).content)用with进行上下文管理不需要人为的关闭文件，一旦程序退出with模块文件会自动关闭。本地地址可以随便换，这…显示全部
想了一会儿要不要回答，因为题目被贴上了作死标签……q(°A°`)r --------------------------------------------------------------- Chrome大法好，Python大法好。 @林灿斌 说的方法没错，不过一般这种网站的Cookie生效时间都比较长，所以你直接复制你Re…显示全部
你可以试试直接使用开源的爬虫库scrapy，原生支持多线程，还可以设定抓取速率，并发线程数等等参数；除此之外，scrapy对爬虫提取HTML内容也有良好的支持。 网址：http://scrapy.org/ 中文入门教程也已经问世，可以Google一下。显示全部
引用某位大神的话，学习Python没有比文档更好的教程了。 如果是爬虫路线的话： 1.先熟悉语言，看python官方文档http://docs.pythontab.com/python/python3.4/introduction.html 2.爬虫库文档，包括requests和beautifulsoup http://cn.python-requests.org/z…显示全部
#前言#并不是自问自答。#准备计算大V关注与问题是否为热点的相关性。待更。 #预实验脚本写错了…滚去修改。 #早上还好使晚上就405了。见鬼…… #对不起刚刚脑残了，脚本还能用，不过我不打算把它贴出去了，太占地方。想看的私信吧（其实是因为粗糙到拿不出…显示全部
(python3.4)本回答针对该验证码的识别,主要分以下五部分:验证码分析下载验证码样本生成初始字符图片样本识别验证码测试与总结 涉及的脚本及文件(脚本按照出现的先后顺序排列).../captcha_char.../captcha_example.../download_captcha_0.py.../download_cap…显示全部
binux/pyspider ・ GitHub 通过使用 PhantomJS 渲染，然后 dump 页面 HTML 的方式和 python 进行交互。这样可以充分重用 python 的设施。 pyspider 爬虫教程（三）：使用 PhantomJS 渲染带 JS 的页面 （这与百度的思路是类似的）显示全部
我用R语言来实现这个：library(XML)url<-"http://hz.house.ifeng.com/detail/2014_10_28/50087618_1.shtml"tbls<-readHTMLTabletbls<-readHTMLTable(url)sapply(tbls,nrow)pop<-readHTMLTable(url,which = 1)write.csv(pop,file="F:/pop.csv") 最终爬…显示全部
我所列出的次序是按照在百度中搜索“代理IP”所得出的次序，排名靠前并不能说明质量就靠谱！―个人观点，当然这些也大部分是我亲自使用体验过的1.西刺代理――大象代理搜索代理IP，首先跳出来的是西刺代理。西刺免费代理ip页面所提供的代理几乎都不能用，相…显示全部
在临时链接后面加上 “&devicetype=Windows-QQBrowser&version=61030004&pass_ticket=qMx7ntinAtmqhVn+C23mCuwc9ZRyUp20kIusGgbFLi0=&uin=MTc1MDA1NjU1&ascene=1” 302重定向到永久链接 location就是永久链接显示全部
一个行业越成熟，分工就越明确，整个链条也越稳定。每个行业都有“矿工”，我是铁定了心只做大数据矿工，把爬虫做的最好用。 就像一部智能手机，高端的技术浓缩在几个芯片中，研发不成那个芯片，能做成富士康也很了不起。 所以，最重要的是选定行业位置，站…显示全部
从整体搜索引擎角度，分成三个子系统：爬虫(URL管理和调度下载解析等）、索引(用于全文检索)、存储(解析后的内容和快照等)； 爬虫子系统部分，又分为四个组件：Seeder- > Manager- > Harvester- > Collector(- > Seeder). 1) Seeder负责URL管理，也就是定向…显示全部
看到理想这个词就滚进来了，终于可以给别人谈谈理想了#到我办公室来，我们聊一下未来# 1.分布式的话，那就有足够强壮的爬虫调度，快速的横向部署扩展加入节点，任务到达保障，容错等等基本要稳定好。 2.抓回来的数据，如果有需求的情况下，应该离线下来，包…显示全部
无论是Python也好，java也罢，凡是爬虫，你爬下来的东西有百分之九十九的概率会存下来，当然，如果不存下来那就别继续往下看了。 首先，爬下来的数据保存到数据库或者其他地方。 其二，使用集合构建一个缓存，自己看内存，数据多少定大小。 其三，重新启动…显示全部
老实讲你上面讲的这些，大部分都是有些道理站得住脚的。不过，你忽略了一个重要因素――大部分「大数据创业公司」事实上本身是没有多少数据的。 无论一家公司标榜自己数据分析水平多么牛，没有数据，你分析什么呢？毕竟 BAT 这样海量数据的公司数量有限，剩…显示全部
本身robots.txt只是一个约定，是道德约束，一般的搜索引擎爬虫都会遵守这个协议的，否则如何在这个产业混下去。而且robots.txt一般也是站点为了更好被搜索引擎收录所准备的。 真正的封禁不可能靠user-agent来封禁，因为spider如果携带正常浏览器的UA理论上…显示全部
说起亚马逊采集，我算实战经验非常丰富的老司机了，总体来说，想要完整、大量、精准的、稳定的采集亚马逊数据，其实并不容易，当然，这取决于你选择什么方式，工欲善其事必先利其器，好在我们有“大（八）杀（爪）器（鱼）”。国内很多在亚马逊做外贸的企业…显示全部
import requestsfrom bs4 import BeautifulSoups = requests.Session()r = s.post('http://www.zhihu.com/login', data={'_xsrf': BeautifulSoup(s.get('http://www.zhihu.com/').content).find(type='hidden')['value'], 'email': 'email', 'passwor…显示全部
Headless Webkit，开源的有 PhantomJS 等。 能够解析并运行页面上的脚本以索引动态内容是现代爬虫的重要功能之一。 Google's Crawler Now Understands JavaScript: What Does This Mean For You?显示全部
把别人网站的信息弄到自己的电脑上，再做一些过滤，筛选，归纳，整理，排序等等，如果数据量足够大，算法足够好，能给别人提供优质的检索服务，就可以做成类似google或baidu了显示全部
首先，数据大了，存储绝对不是一件容易的事，要考虑很多因素。 爬虫爬下来的大量数据，存在关系型数据库里往往不是很恰当的，因为当数据量和并发很大时，关系型数据库的容量与读写能力会是瓶颈，另一方面，爬虫保存的页面信息之间一般也不需要建立关系。 比…显示全部
一般来讲,爬取需要登录的网站有两个思路: 1)模拟登录, 2)使用登录后的Cookie. 先讲讲第一种方法吧, 模拟登录这个网站. 使用FF或者Chrome, 可以轻松地发现这个网站的登录其实就是向https :/ / www. appannie. com/ account /login/ 这个网址POST一些参数(由…显示全部
如图…… 这样就可以先用BS。 最后得到的结果加上 .decode('unicode-escape') 就可以正常转中文 第一次见到 .decode('unicode-escape') 简直感动…显示全部
因为爬虫教程全是在教爬知乎、豆瓣（滑稽.jpg认真讲就是，事实上爬知乎、豆瓣的人确实比爬 QQ 空间的人多，一个原因是难度不一样，尤其新版知乎页面简直太爬虫友好了，所谓的「爬知乎」，简直就是学习怎么发请求，所以知乎改版后再写爬知乎教程的人，我都非…显示全部
根据我的搜索,用这个帖子(http://kevinsfork.info/2015/07/23/nwmusicboxapi/)算出来的的参数去post这个地址 (http://music.163.com/weapi/v1/resource/comments/R_SO_4_30953009/?csrf_token=) 就应该能得到评论的json数据 代码大概这样子 #!/usr/bin/env python# -*- coding: utf-8 -…显示全部
兄弟，你有去试着写过一个爬虫吗？那我来教你好了。 我点开了你给我的网址，点了你所说的《进口分贸易商明细》，看他的url并没有改变是嘛？ 0 0 我k 为什么，这么诡异，那我们用chrome 按下F12看看他到底搞了什么鬼 ，如下图： 奥，原来他这是一个内嵌页面…显示全部
给定目标类型的页面，如何选择最有效的抓取路径？ 如何保证覆盖率？ 给定目标页面，如何最有效的调度，保持更新？ 如何判断 url 不同，内容相同的页面？ 如何在不抓回页面之前判断内容相同页面？ 不给定目标页面，如何判断页面是否有价值？ 如何在不抓回页…显示全部
前几天刚刚爬完了一百多个好友的总计5万条说说，说一下自己的思路。 QQ空间的空间动态都是由JavaScript控制的(电脑和手机触屏版皆是如此)，自己便选择了Selenium作为对数据获取的工具。 总的工具： Python+Selenium+Firefox+MongoDB 总的过程为： 1、从QQ邮…显示全部
教程是python2.x的，应该是老教程了。Python的2.x分支版本在2020年将停止更新，同时，Python 3将被定为该语言的未来发展方向。还是学Python3打好基础比较好。 我来给一下我的学习路线给题主参考一下吧： 基础：Python 3教程发布（ @廖雪峰 廖老师的教程挺易…显示全部
这个官网给出解释以及解决办法了： 在setting文件中设置：FEED_EXPORT_ENCODING = 'utf-8' 就可以啦显示全部
http://github.com/zhu327/rss 既然你也用python就直接看代码吧 爬这里 http://service.weibo.com/widget/widget_blog.php?uid={uid} 替换uid,无需登录,不会被挡显示全部
跟着 @egrcc 大大跑来的 关于取不到内容的原因大大已经解释清楚啦，应该就是登录需要验证码的问题。 以下是我项目中登录知乎（包括处理验证码）和 Cookies 管理部分的代码，供你参考，希望对你有所帮助（不保证新鲜程度，如果无法登录的话，请到 Github 查…显示全部
非常感谢 @没有人 啊。 根据他自己说的…… 似乎是写了个python脚本，抓了知乎中妹子集中爆照的问题…… 然后他把女生替换成了伪娘，再进行提问，花式骗照。 值得夸奖的是，他在无意中，或者有意地，在知乎营造了一个正常化、无歧视、开放、包容的环境。 因…显示全部
突然想把这个答案更新一下 最近一直用python写代码。之前学习过python的数据分析，也算是从0单排的python。 其实学习python，更推荐直接从项目 project 入手。 通过做成一件事来学习，我觉得写程序这东西 通过看书，看多少忘多少。 之前python 的list dicti…显示全部
公司技术咨询按小时收费。显示全部
Q1.通过什么技术方案，可以时时爬取？ 单机爬虫我相信对于一般的程序员，完成都不是很困难的，更何况你只需要爬一个网站，人工写是一种很锻炼图遍历操作的好机会~~~我相信找几个优秀的大二大三学生就可以完成。。。不过，我们既然是程序员，就要学会偷懒，…显示全部
楼上已经有一个答案了，我来另外一个思路的吧： 还记得浏览器F12么？打开题主的游戏话题页面 注意看右边的方法是GET 然后回到页面，滚动到最下面，让页面自动刷新内容： 这个时候，调试模式中，多了一个HOT出来，这次是用的POST了，而且headers内容多了一些…显示全部
Pyspiders是国内某大神开发了个WebUI的[Pyspider](GitHub - binux/pyspider: A Powerful Spider(Web Crawler) System in Python.)，具有以下特性： 1. python 脚本控制，可以用任何你喜欢的html解析包（内置 pyquery） 2. WEB 界面编写调试脚本，起停脚本，…显示全部
学习爬虫技术很多年了，现在自己做了个标准化采集器――发源地。下面来解答下你的疑问： 1、要想了解爬虫，那么，不得不提到HTTP协议了，HTTP是爬虫工作的重要支撑，了解HTTP协议对于我们写出强大的爬虫有着重要的意义，如果你现在对HTTP协议还完全不了解，…显示全部
Python入门当然建议直接学Python3了，毕竟是趋势。 而且Python3中对于字符编码的改动会让新手省掉很多很多很多关于字符编解码问题的纠结。 另一方面看你项目大小吧。 如果自己写代码，Python2和Python3没啥区别。 但是如果你是奔着Scrapy这个爬虫框架去的（…显示全部
其实搜索很多生活中你要想要知道的东西 百度都很厉害 搜索学术相关的什么资料啊 外国的东西等 当然Google厉害 百度主要是面向中国用户。可能会比较看重中国市场。 因为它是全球最大的中文搜索引擎！ 国外的用户习惯用google 转载自 百度网站在外国有吗?_百…显示全部
京东的商品价格是Javascript请求了数据之后“填到”html里去的。爬虫response的html里是解析不出价格的。 两种思路： 1.像浏览器一样运行js，解析js运行完成之后的html。 如果是用Python，Selenium、Phantom JS之类可以，还有一个轻量级的Ghost.py 缺点很明…显示全部
我是一直主张在爬虫中嵌入一个浏览器，现在用python编程，实现这个方案是很容易的，有很多浏览器driver。内嵌浏览器的话，相当于模拟人的浏览行为，网站的屏蔽爬虫的措施可以避开一些，有些网站不是为了屏蔽爬虫，但是从会话的完整性方面会要求传递一些http…显示全部
我认为是解析网页的时候网页编码的问题 显式地指定收到的网页编码为‘utf-8’或者自己转换一下就好了 urllib这个库基本没用过，推荐requests这个库，相当好用 import requestsurl="http://www.starbaby.cn/zhinan/609987"req =requests.get(url)req.enco…显示全部
我写一份完整版的吧包含selenium+phantomjs和selenium+chrome的 留了一份博客版的：selenium设置chrome和phantomjs的请求头信息 | | URl-team 目录一：selenium设置phantomjs请求头：二：selenium设置chrome请求头：三：selenium设置chrome--cookie：四：se…显示全部
。。换了UA有啥用。。又不是识别不出你来。。就好像你在超市领赠品，每人只能领一次，你换个衣服再去。。。人家不但认得出来，还觉得你滑稽显示全部
爬虫需要处理的一些问题： 发送HTTP请求 解析HTML源码处理反爬虫机制效率...... 说到底就是因为Python在处理上面的事情的时候有很多库，而且语法简洁，代码风骚，开发者的注意点可以集中在自己要做的事情上。 ---------------------------------------- 简…显示全部
谢邀，下次这样问题别再邀请我。 这是逗逼？ 你以为爬虫是毒龙钻？ 还爬数据库？服务器日志？先搜索&&搞明白爬虫定义。 某百科：“网络爬虫是一种自动获取网页内容的程序，是搜索引擎的重要组成部分。” 是网页！网页！网页内容！！！！！ 小伙子，我看你骨…显示全部
简单来讲，你通过代码向服务器发送的请求与浏览器向服务器发送的请求不一样，所以你可以通过浏览器获取数据，但是无法通过代码。 首先建议你打开浏览器的开发者工具，推荐使用Chrome浏览器。选中Network一栏之后重新通过浏览器向服务器发送一次请求，然后，…显示全部
@sync in， @三木小森 @赵云鹤 最近帮一位珠宝商朋友实现了公众号的爬取工作，不通过搜狗获取，基本是全自动化操作。目前已找到数千公众号，数十万文章。但搜狗微信里面珠宝公众号只有189个。 以珠宝为关键词找到的文章也太多太繁杂，所以配套着还做了个搜…显示全部
一、如何获取http://a.com中的url，同时也获取http://a.com页面中的数据 可以直接在parse方法中将request和item一起“返回”，并不需要再指定一个parse_item例如： def parse(self, response): #do something yield scrapy.Request(url, …显示全部
想起前几天刚在公众号深度分析了拉勾网上面的求职信息，正好写在这里和知友们分享。感兴趣的知友们欢迎关注公众号：城南的数视界。 首先列出一些结论给没耐心看完全文的知友分享：一线城市（北上广深）的求职信息数量领先全国；一线城市（北上广深）对人才…显示全部
如果不方便手动加u的话，像这样显示全部
当年写了个java程序抓了400亿条tweets. 实现基本api调用方法后要解决如下问题: 1. 多线程平行运算以及动态规划抓取对象 2. 搜索并管理大量代理服务器来规避官方api限制 3. 搭建一个12tb的硬盘矩阵并优化java sql界面来存储(有钱的话这一点可以忽略, 用云服…显示全部
不请自来。 Python写这些爬虫，批量获取还是挺简单的……不长逛P站但是写了个可以获取知乎指定收藏夹下的高赞答案的东西…… 获取知乎指定收藏夹下的高赞答案 获取P站特定标签下的高赞作品 这两个需求应该是差不多的…… 爬取收藏夹的代码大概是这样： __au…显示全部
你这个爬虫跟JS关系不大，直接看Network，看发出的网络请求，分析每个URL，找出规律，然后用程序来模拟这样的请求，首先要善于用Chrome的Network功能，我们点几页，看Network如下： 第一页数据URL： http://q.10jqka.com.cn/interface/stock/fl/zdf/desc/1/…显示全部
你这个地方慢是因为使用HttpRequest有个限制并发数，ServicePointManger.DefaultConnectionLimit这个值默认2你改大一点比如512应该会快很多显示全部
爬虫代理IP池在公司做分布式深网爬虫，搭建了一套稳定的代理池服务，为上千个爬虫提供有效的代理，保证各个爬虫拿到的都是对应网站有效的代理IP，从而保证爬虫快速稳定的运行，当然在公司做的东西不能开源出来。不过呢，闲暇时间手痒，所以就想利用一些免…显示全部
一种是分析爬虫特征，尝试过滤爬虫的请求 另外一些可能技巧： 1. 在页面开头放上一些钓鱼的链接（一般人点不到），爬虫会去访问，一访问就把对应的ip封了 2. 页面全是图像 3. 页面内容用javascript来生成 4. 页面上不提供可以供爬虫追踪的链接，跳转都用js…显示全部
有是有，但是没有国服的= =、、国内的第三方应用的数据我估计有3种途径： 1. 少数与腾讯有合作，直接获得数据2. 通过盒子类的工具自行记录3. 爬了别人应用里的数据 下面主要说说Riot Games的官方REST API。最近我和另外两个小伙伴刚开始一个英雄联盟（LOL）…显示全部
我认为不需要，因为你现在通过网页，不登录别人的账号你也可以知道人家follow了谁显示全部
作为IT桔子的创始人，投资数据是我们非常重要的一个信息模块，爬虫抓取是方式之一，但很难成为最重要的方式： 1）抓取最重要的是数据来源，目前国内可以系统利用的数据源不太多，大概有清科、投中、一些媒体、还有各个投资机构的官网。我们自己在做 IT桔子…显示全部
@Z Nerd回答对了，不过估计你们也不会去man wget 看使用方法的，我给你们一个可用的吧： wget -c -r -np -k -L -p http://blog.hesheyou.me显示全部
可以看下崔庆才老师的 Python3爬虫三大案例实战分享猫眼电影、今日头条街拍美图、淘宝美食 https://edu.hellobi.com/course/156课程源码：今日头条：https://github.com/Germey/TouTiao淘宝美食：https://github.com/Germey/TaobaoProduct猫眼电影：https://github.com/Germey/MaoYan显示全部
response.xpath(u'//span[./text()="出版社:"]/following::text()[1]') 如果text() 中有空格, 感谢 @董成良 提醒, 你可能还需要这么写 response.xpath(u'//span[contains(./text(), "出版社:")]/following::text()[1]') 或者全匹配: response.xpath(u'//sp…显示全部
网站根据cookie ip和header其他部分判断用户的 可以把你浏览器的cookie导入python的爬虫里，firefox是以数据库的方式存储cookie，要用sqlite读一下，ie是按不同网站分不同文件存储。 导入对应的cookie之后，如果爬虫被封，就进对应的浏览器填个验证码。 这…显示全部
一般有几个办法可以拿到视频的原始url： 1）可以试下傲游浏览器，傲游浏览器在播放一个视频页面时，右上角有个获取本页上视频文件的功能，原理是捕获整个浏览器的所有http请求数据包，如果有.mp4/.flv等文件，则记录下来地址。这种是比较简单和兼容大部分视…显示全部
推荐如下的java开源爬虫或抓取框架 1.webmagic 【猪猪-后端】WebMagic框架搭建的爬虫，根据自定义规则，直接抓取，使用灵活，Demo部署即可查看。 官站：WebMagic 2.jsoup java网络爬虫jsoup和commons-httpclient使用入门教程实例源码 搜索"jsoup"的分享列表…显示全部
在页面里放上反爬虫工程师的招聘广告 又想起来一个…直接办阿里腾讯vps的公网ip端显示全部
首先要自己会写代码。 学习爬虫可以从下面一些知识点入手学习。 1、http相关知识。 2、浏览器拦截、抓包。 3、python2 中编码知识，python3 中bytes 和str类型转换。 4、抓取javascript 动态生成的内容。 4、模拟post、get，header等 5、cookie处理，登录。…显示全部
没有所谓的「验证码库」。 常见的验证码都是通过程序实时生成的：随机生成 N 位数字、字母，然后通过某种算法扭曲、旋转或变形之后依次画到背景上，最后再生成一些干扰线和噪点，就做出了一张验证码。 理论上讲，你可能遇到两张验证码图片的内容相同，但是…显示全部
如何抵御反爬虫机制，比如何高效抓取数据更加重要。显示全部
推] libcurl - the multiprotocol file transfer library显示全部
只要你能用浏览器正常访问，我就能爬到。 这不只是一句slogan，而是你到这个水平才能写上去的. 爬虫嘛，往复杂了无非就是多机器多节点超大规模数据的分布式抓取，或者超级难防的反爬虫机制，经验活儿~~~显示全部
你大概爬了知乎吧，我也在爬知乎，但是速度没你那么快，可能是我不会用并行爬虫吧。 贴一个我用quora数据做的研究，有兴趣我们可以一起合作。 作者：江汉臣 链接：如何在Quora上获得更多的赞――来自10393个回答的实证 - 防川 - 知乎专栏 来源：知乎 著作权…显示全部
谢邀。先上结论，通过公开的api如果想爬到某大v的所有数据，需要满足以下两个条件： 1、在你的爬虫开始运行时，该大v的所有微博发布量没有超过回溯查询的上限，新浪是2000，twitter是3200。 2、爬虫程序必须不间断运行。 新浪微博的api基本完全照搬twitter…显示全部
谢邀 高校论坛不能一概而论。 我先来说最普遍的吧。论坛自然是Discuz，关于Discuz!X2的登陆代码和发帖网上有太多了，一搜一大把。我们学校的内网bbs用的是Discuz!X3.2，我来聊聊爬内网BBS的过程。外网是访问不到的，所以你们也不用尝试了，抛砖引玉，可以去…显示全部
其实有一个很取巧的方式，2014年的时候我就回答过。 百度URL加上参数pn=750，访问并抓取这个页面，在源代码找到pageNum，即可知道有多少页。 举例如下 https://www.baidu.com/s?wd=%E5%B1%B1%E4%B8%9C&pn=750 山东 有 76 页。 https://www.baidu.com/s?wd=%E6%9B%BC%E9%99%80sp%E5%BA%84%E5%9B%AD%E5%A7%9C%E5%88%91&pn=750 曼陀sp庄园姜刑 有 20 页。显示全部
看来你们学校教务系统和我们学校教务系统都是用http://ASP.NET开发的。 http://ASP.NET每个页面的VIEWSTATE是用来保存网页信息的，通过这些信息可以知道网页中各控件的状态。所以为了能够不出差错，最好是从登陆页面起，每次提交表单的时候都先…显示全部
用av2047063举例，访问下面的网址：【网址已隐去】 @妹空酱 提醒我才想起来。。。。 先去自己申请一个appkey。。。在这里： bilibili - 提示 然后就可以对bilibiliapi为所欲为了。。。。 B站第三方客户端就是这么开发出来的。。。 可以看到最后两个参数id=a…显示全部
亲测 网页插入代码  <p> 如果说日子是一天一天过，不知道明天怎么样？ <script>document.write("明天会更好");</script> 好吧，我承认。 </p> 快照更新，打开百度快照，看源码： <p> 如果说日子是一天一天过，不知道明天怎么样？ 明天会更好 好吧，我承认。…显示全部
专栏：拓端数据研究院由于电商网站的数据的实时性要求，数据分析时一般直接从网页爬取。因此使用爬虫的方法显得十分重要。R作为数据分析的软件，可以直接对爬取的数据进行后续处理，加上上手快的特点，是电商网站数据爬取和分析的好工具。 下面以http://cn.…显示全部
谁让你用一个IP了 我每次爬前，都会先去爬代理网站 抓几百个代理IP 然后验证可用的和匿名的 封装一下 谁用谁知道显示全部
好多评论讨论的朋友为啥都不点个赞呢。。――――――――谢邀，话说这一年前的问题应该是观众还没有从别人的回答中看懂吧，其实三不青年的写法是对的只是加了login_data，这段时间刚好总结这些基础的反反爬技术。有兴趣的可以看看我这个开源项目：Anti-Ant…显示全部
感谢 @梁霄 的回答，我今天也遇到了跟楼主类似的问题，疑似accessKey泄漏，黑客在我的帐号下疯狂创建EC2实例造成将近700刀的billing，Forums上提交了个问题，没多久就先有邮件确认回复了，过了一会又有一老外打电话跟我咨询问题，居然还会说中文。。前后通…显示全部
本人擅长Ai、Fw、Br、Ae、Pr、Id、Ps等软件的安装与卸载； 精通CSS、JavaScript、PHP、C、C++、C#、Java、Ruby、Perl、Lisp、Python、Objective-C、ActionScript等单词的拼写； 熟悉Windows、Linux、MacOS、IOS、Android等系统的开关机，求一份月薪上万的工…显示全部
首先Unicode不是乱码，如果把这些数据保存下来，或者直接显示使用是没有问题的。 另外，如果你需要在scrapy shell中调试hxs的时候，你可以自定义一个printhxs(hxs)这样的函数用来在shell中显示中文。 def printhxs(hxs): for i in hxs: print i.encode('u…显示全部
智能镜子。。还在进行中。 就是这货 界面是python写的 tkinter框架。 显示全部
这个官方文档上有说明的，不同的解析器之间是有差异的。 以下来自官方文档: Beautiful Soup 4.2.0 文档 解析器之间的区别 Beautiful Soup为不同的解析器提供了相同的接口,但解析器本身时有区别的.同一篇文档被不同的解析器解析后可能会生成不同结构的树型文…显示全部
自己动手做了一个爬取京东商品评论的 下载地址http://www.yunya.pw/?post=4 ==================================== 爬虫的Python源码，陆续整理... 云涯 - Personal Web 天猫旗舰店 - 2016.11.25 京东旗舰店 - 2016.11.27 ====================================显示全部
提供个思路给你： 这样再试试。网页端检测是不是手机浏览器的方法，一般除了User-Agent就是视口宽度了。显示全部
没有解决吧显示全部
遇到这种问题从来都是一张图 显示全部
谢邀，今晚先写写关于cookie的部分，手机打字慢，先占个坑。 话说，，光有感谢怎么没人赞。。虽然这种纯技术问题一般比较冷门吧，好歹点了感谢的顺手来个赞，，激励我一下。。 ###################正文开始的分界线################### 首先，爬取网站的时…显示全部
先监听请求，发现是这个链接 /cnhp/life?IID=SERP.{0}&IG={1} /cnhp/life?IID=SERP.5044&IG=A57C331E82D44479BBA57F5BF42349BA 出现的文字介绍，问题来了，这个IID和IG从哪里来的？ 只有strfind+debug找JavaScript相关代码。 如图: 如此便得知，IID位于源码…显示全部
20160503 修改了代码缩进，刚玩知乎，当时回答的时候点了插入代码，貌似没生效。谢谢 @iliul的修改的格式 ------------------------------------------------------------------------------------------------------------ 使用Response类的接口iter_conte…显示全部
新浪微博在八爪鱼里的登录需要一定的判断条件，对于不熟悉八爪鱼的人确实存在一定困扰，这里有个八爪鱼的私人小技巧推荐给楼主，文末有彩蛋哦，这里先讲讲方法。我用的方式也是预先登录的方法，先制作一个简单的打开新浪微博并随意采集一个字段的规则如下：…显示全部
1. 调用百度的API即可。基于你说的情况，应该需要的Place API及Geocoding API，个人开发者经过认证之后前者每天有10万次的请求次数，后者有100万次的请求次数，足够你调用了。文档在这里：http://lbsyun.baidu.com/index.php?title=webapi 2. 由于某些原因…显示全部
微信机器人，先感谢一下框架开发者： @LittleCoder，需要框架请找去他的github 用littlecoder老大的微信机器人框架，做了一个股票量化助手的功能： 目前功能： 比如：其中的一条选股功能。 再来一个复牌功能： 热点题材： 龙虎榜整理： 显示全部
https://github.com/alsotang/node-lessons/tree/master/lesson3显示全部
说了这么多都这么麻烦，我分享一个立马可用的给大家。先看一下采集微信文章的效果图，这个是别人做的一个采集规则，先不用管什么是采集规则，反正这个图片中那个规则试用的按钮一点，微信文章的标题正文阅读数等都会被采集成excel这种格式的数据表格，具体…显示全部
难道不是搜索引擎吗。。。。显示全部
# encoding=utf8# author:shell-vonimport requestsimport reaid = '3210612'api_key = "http://interface.bilibili.com/count?key=27f582250563d5d6b11d6833&aid=%s"data = requests.get(api_key % aid).contentregex = r"\('(?:.|#)([\w_]+)'\)\.…显示全部
涉及到文本的话，基本是dirty work了。。 1. 爬虫就不要想实时了，这些旅游网站的攻略信息都是UGC的，全站内容都没多少。先花点时间爬全站，然后每天爬新产生的就好了。框架很多，py的有scrapy pyspider之类的，不用框架直接写也凑合。 2. 从攻略信息里要提…显示全部
直接用pandas啊，简单方便 import pandas as pd data =pd.read_html(url)[0]显示全部
动态网页一般两种思路 ，一是找到api接口伪装请求直接请求数据，另一种是没有办法模拟的时候需要渲染环境，具体怎么做和代码，请参考博客(scrapy下的js 渲染环境，官方已经给出解决方案)。 (9)分布式下的爬虫Scrapy-关于ajax请求的分析应该如何做 (10)分布…显示全部
给你看下我们团队做的岗位调研，python web后端开发偏多一些。第一部分：各个领域应用的语言。 大家看这个内容，其实你很明显发现，其实各个语言都有他的用处。我们可以说Python是应用最广的。但是暂时还是不能说它是全能的，因为他也有它的短板，但是对于…显示全部
这个东西的优势是近乎万能，缺点是成本太高。显示全部
答案是可以做到的，事情是这样的，我和几个同学打算做一个针对大四毕业生的网站，所以需要整合学校官网的校园招聘信息（只抓取校园招聘不要网投简历的）我在抓取学校官网的校园招聘信息时就遇到了这个问题。我采取的方法是抓取图片的链接，这个链接一般是没…显示全部
楼主节哀。Email的地址我不是很清楚，但是你可以通过去AWS Developer Forums里面反映情况（我google了“aws ec2 forum” 关键字，很容易找到），官方论坛里面回复还是很快的（至少我们组还是很认真的看论坛里的反馈）。 个人认为你需要说清楚的点： 1. 明确…显示全部
首先有三个最需要解决的问题： 法律和道德风险：爬虫抓取其它网站数据，虽然抓取的内容大部分是公开的，但是商用或者有损源网站利益，于法于理都说不过去。目前我国（或者说大部分国家）针对互联网的方方面面法律覆盖度还远远不够。 访问速度与瓶颈：爬虫的…显示全部
不想用现成库的话，你可以先试着实现一下求解：w = (x ^ y) mod z，其中： x = 0x251efe513c7f8db2b395bde03767facda67109811619160e537b241d02ef639d;y = 0x1e5a85b28c1a3ed0695bdb8ab15cfa5737933cc84f65cfee3f27956967216a49;z = 0x22de8ed23bf3f2ddf3…显示全部
>>> url = r'http://m.baidu.com/from=1012585f/bd_page_type=1/ssid=15567761646568656174327407/uid=0/pu=usm%402%2Csz%401320_2001%2Cta%40iphone_1_3.0_3_528/baiduid=F8CF1C9B13A69F341A9A38C3718410D8/w=10_10_%E9%B1%BC%E8%82%9D%E6%B2%B9%E7%9A%84%…显示全部
针对题目，我有两个建议： 1. 使用xpath或者css selector替代正则。 2. 使用gevent来提升爬取速度。 感谢 @松鼠奥利奥 提到我，发现当初回答这道题是用手机随手一答，并没有说清楚具体原因，抱歉。 为什么建议使用xpath代替正则？ 对于固定场景，合适的正则…显示全部
爬虫这种IO密集型，而且是对外网的请求，如果需求的QPS 超过500/s，就用erlang，golang或者其他语言的epoll库。否则就启N个consumer线程处理就好了。爬百度的数据，一个机器的ip肯定是不够的，先从淘宝上买一堆的http代理肉鸡ip回来再搞。 你这个需求，处理…显示全部
谢邀，我只是NLP方面的初学者。 首先贴一个干货： Python 网页爬虫 & 文本处理 & 科学计算 & 机器学习 & 数据挖掘兵器谱 然后你的需求转变为技术性语言就是：如何通过数据挖掘手段给文本打标签？ 可以通过分类和聚类两方面下手，比如构造一棵话题树（知乎就…显示全部
In [1]: from urllib.parse import unquoteIn [2]: unquote("%C0%FA%CA%B7%C9%CF%C4%C7%D0%A9%C5%A3%C8%CB%C3%C7.PDF", encoding="GBK")Out[2]: '历史上那些牛人们.PDF'显示全部
你看看get_image函数里的for循环体。。有两行缩进不对吧。。 另外有几个问题： 1. 你Python该升级了 2. 不推荐用正则表达式解析url。推荐用lxml或者BeautifulSoup 3. 不推荐用urllib做网络请求。推荐用requests显示全部
要躲别人的查封基本不太优雅，感觉用代理最简单，打一枪换一个地方，以前测试过go agent抓豆瓣： #coding=utf-8import urllib2import urllibimport cookielibfrom bs4 import BeautifulSoup proxy_support = urllib2.ProxyHandler({'http':'http://12…显示全部
这验证码毛都没验证，产生在浏览器端，验证在浏览器端，服务器一端啥都不知道，要来啥用？ 你发送表单的时候随便甩四个字符填个字段发出去就好，管它验证码是个毛。显示全部
看看phantomjs和selenium显示全部
不太清楚豆瓣防屏蔽的具体机制 我之前抓豆瓣的数据也一直被403，后来把程序放到SAE就万事大吉了。。。 好吧，打个小广告 豆瓣各种热帖 豆瓣直播帖 豆瓣直播热帖 实时脱水版显示全部
问题已经解决： 具体是卡在这里： 1是为了图方便，下载图片用了urllib.urlretrieve， 可是cookie时绑定到urllib2上的，所以获取cookie失败 2是发现登录知乎必须对login/email接口提交两次，第一次提交即使captcha对了也不行，必须两次，太坑了 3就算是登录…显示全部
各位别逗了，跟据各位的答案，除了 @woshi123 是真的被反爬虫命中之外，你们乱码都是代码写的渣好吗~~~显示全部
准确, 实时和免费这三者之间只能同时满足其中两个啊.... 如果要自己爬的话, 推荐使用http://finance.yahoo.com的api了. api很全, 支持csv导出. 您业余想做的这个轮子, 其实市面上已经有很多产品了: 国内的: https://app.wmcloud.com/ 国外的: Quandl - Find, Us…显示全部
从易到难，提几种方式： 1.多终端运行：开sshd服务，安装xshell，多开些会话，人肉； 2.后台运行：nohup xx.py； 3.守护进程方式：uwsgi/gunicorn/supervisor； 4.docker方式：docker镜像，运行container； 开发&运维&安全交流，请关注知乎号。显示全部
不邀自来，刚好看到顺手试了一下，也讲一下怎么去检测网站反爬虫特性吧。 对一个新网站一般 直接用scrapy shell 来测试基础特性 scrapy shell http://www.jianshu.com/p/79c80cee94cc 然后返回信息是： 这403并不是说找不到，一般对面不让你访问到这个资源…显示全部
1. 买一个vps。通过web方式，给vps装上操作系统，如ubuntu。这个供应商会有专门教程。 2. 用ssh 登录到vps上，然后把你的代码放上去。然后运行“nohup python ./youcode.py &”，于是你的crawler就会持续运行下去。将转到的数据存在文件里，这个是你代码里…显示全部
这个是动态生成的页面，如果你查看网页源代码的话，是看不到截图中的那些HTML代码的。而Python爬取动态页面的方法之一，就是模拟浏览器去访问这个页面，然后获得页面的HTML后再进行解析。具体来说，需要用到Selenium、BeautifulSoup4。 pip install seleniu…显示全部
爬虫的性能瓶颈通常是网络IO吧，内容解析速度再快也没用。显示全部
pycharm显示全部
相识于十二岁，马上就是第十年了。 喜欢了四年，后五年断了联系。拉黑好友，断了一切联系。然而九年以来他的手机号码都没有换，搜到他的微信，戳进他的朋友圈发现他有新的生活。中间这五年发生的什么我一无所知。 我也曾无数次幻想，加他之后告诉他我真的已…显示全部
谢邀，看到问题好久了，一直怕误导别人，没有答，今天睡不着，和你分享一下我的看法。毕业两年，结合自身经验来说下吧。 如果你是编程初学者，可以试着不使用框架，从语言的原生接口上面写一套标准的HTTP API出来，这个还是比较考验功力的，你可能需要好好…显示全部
爬手机端 http://weibo.cn 可以参考下面的代码，来自极客学院，侵删 #-*-coding:utf8-*-import smtplibfrom email.mime.text import MIMETextimport requestsfrom lxml import etreeimport osimport timeimport sysreload(sys)sys.setdefaul…显示全部
现在的爬虫是百花齐放，各种工具都有 python:scrapy,pyspider Java:webmagicGolang: Pholcus.net : abot .......但是从实用性和易懂的角度 ，python 你可以首先考虑，因为python入门容易，scrapy的社区活跃，出了各种问题都可以找的到答案。 关于教程的话，…显示全部
谢邀...其实这个看requests官方文档就好，或者百度一下。import requestsurl = 'http://52kantu.cn/static/photos/full/5f483af86df016c855cb9af9b76c7f4271f700c2.jpg'r = requests.get(url,stream=True)with open('123.jpg', 'wb') as fd: for chunk …显示全部
引用下其他的人回答，希望对你有用。 网络爬虫是人饕擎抓取系统的重要组成部分。爬虫的主要目的是将互联网上的网页下载到本地形成一个或联网内容的镜像备份。下面主要对爬虫以及抓取系统进行一个简单的概述。 一、网络爬虫的基本结构及工作流程 一个通用…显示全部
谢谢邀请回答，我使用八爪鱼也很久了，知乎上也写过各种回答。包括用了其他软件之后，甚至都快可以写出国内采集界的行业竞品分析报告了（别问我为啥不写，因为懒，因为被领导折腾成PPT狗导致没时间…… _(:з」∠)_）。如果不从竞品分析报告的苦逼角度来探…显示全部
楼主这个问题问的非常好，研究了一天才研究好，不过也算是研究了一年吧。 楼上说的都挺神秘的，跟js什么的没有任何关系。 还有用切图做的 如果是我做的话，肯定想不到这么复杂。 间隔一段时间就会换字体，每次字体对应的数字的排列顺序也不一样， 自己定义…显示全部
谢邀。 知乎获取验证码的请求最近做了修改，需要传两个参数： 1. `r`: 随机数，可以用 random 模块生成，或用当前时间戳 2. `type`: 登录时固定为 'login' 所以目前（2016-05-16）该链接的形式如下： https://www.zhihu.com/captcha.gif?r=1463377588&type=…显示全部
在工程中有这样一些问题， 1，快速频繁访问会被封IP，一般可通过代理和增加等待时间解决； 2，需要登录信息，例如微博，可通过携带cookie解决； 3，国内下载国外网站可以用国外代理； 4，网页解析，有比较成熟的各种库，常用的有python语言； 5，正文抽取 6…显示全部
当你运行 scrapy command arg 这样的命令时,这里的 scrapy 实质是一个 python 脚本，它接受参数，首先调用 scrapy/cmdline.py 中的 execute() 函数. 在 scrapy 安装目录下的 cmdline.py 文件中: ...def execute(argv=None, settings=None): if argv is No…显示全部
没用过pyspider，就scrapy而言，我觉得已经足够灵活了，首先它的parse->yield item->pipeline流程肯定是所有爬虫的固有模式。 解析函数可以自己写，怎么处理页面是你自己决定的。 pipeline可以自己写，抓到了数据要怎么处理也是你自己决定的。 剩下的无非是…显示全部
这不是取代的问题。python是语言，爬虫是技术，而且，不仅仅是python，很多语言都可以实现爬虫技术。 但是，你要知道，当要采集、爬取的数据是大量的时候，单机采集是十分缓慢的。 而八爪鱼采集器还提供了云采集服务，在很短的时间内就可以完成你可能需要几…显示全部
谢邀，想了想，可能因为我不怎么和人吵架，一时还想不出能有啥词来评价这**的校方 既然校方是用**的方式来对待你的，你可能也只能用一些**的方式来“更好地解决这件事”，这方式可能不那么酷，但是如果你理解了这背后他们的一些思维方式，以后在**统治的地…显示全部
凤凰军事新闻这类站点还是比较简单的,很少反扒策略。网站数据直接返回，不存在异步请求等。意味着你可以使用httpclient发生请求,直接使用Jsoup解析返回的html页面获取所需数据。你说必须使用java, 万幸Java写爬虫还是杠杠的,大量大牛们给我们这些潘抗毕琢恕显示全部
工商信息主要来源爬虫抓取与即时更新 爬的：工商局不提供任何数据接口，第三方数据库信息为爬取所得爬取信息属合规性质，工商局有义务公开企业信用信息目前各第三方数据库信息抓取方式如下：通过爬取全国各省市企业信用信息公示系统获得部分公司的工商数据…显示全部
神tm注释…… decode不是编码转换，decode是把一个无编码的str，依照给入的参数为解码系统，解码后映射到unicode字符集，返回为unicode对象…… 出错的原因正是因为国内网站热衷于瞎搞，于是一个页面里混了多种编码。如果不是关键内容，解不出来就ignore也…显示全部
谢邀。 楼主的问题是如何高效识别出爬虫，其实最高效的方法就一个：爬虫自己标记自己是爬虫，比如百度和谷歌。 楼主肯定不是想问这样的问题，楼主的问题肯定是这样，有些爬虫伪装成ua在自己的网站上定期爬数据，楼主不希望自己的网站内容被系统性搬运，可采…显示全部
我可以模拟登陆 但是呢 现在手机 回头贴代码 看赞数 哈哈 request.py #!/usr/bin/env python# -*- coding: utf-8 -*-# @Author : jerry.liangj@qq.comfrom config import *from tornado.httpclient import HTTPRequest, HTTPClient,HTTPErrorimport t…显示全部
做了几个小实验，以试图逼近亚马逊的防爬虫机制的边界。 因为各种原因，做得比较简陋，结论都是估计出来的，没有很大依据，漏洞很多，欢迎指出，大家凑合着看 首先大前提是：触发Robot Check的原因是亚马逊觉得你在短时间内大量发出请求，有可能是机器。那…显示全部
题主所说的「隐形链接」在一定程度上是能够防止漫无目的的爬虫的，但是往往是得不偿失的，首先可能会误伤搜索引擎爬虫，不利于 SEO；第二是针对特定网站专门编写的爬虫，根本就不会踩到这个链接；另外有些浏览器可能会预加载某些链接，意味着正常用户同样可…显示全部
对于Python3爬虫抓取网页中文出现输出乱码 import urllib.requestresponse = urllib.request.urlopen('http://www.baidu.com')html = response.read()print(html)上面的代码正常但是运行的时候结果遇到中文会以\xe7\x99\xbe\xe5\xba\xa6\xe4\xb8\x80代替…显示全部
对于结构比较良好的网页，用rvest包效率最高，可以用css和xpath选择器，用管道操作。 hadley/rvest ・ GitHub显示全部
在网络爬虫抓取信息的过程中，如果抓取频率高过了网站的设置阀值，将会被禁止访问。通常，网站的反爬虫机制都是依据IP来标识爬虫的。 于是在爬虫的开发者通常需要采取两种手段来解决这个问题： 1、放慢抓取速度，减小对于目标网站造成的压力。但是这样会减…显示全部
我这儿有个相似的例程，你可以参考下：效果图，在图片上添加文字：python3代码：import osimport randomimport timefrom PIL import Image, ImageFont, ImageDraw# 选择图片img_list = os.listdir("./wallpaper/")img_name = "./wallpaper/" + rando…显示全部
Python 3 和Python 2 语法差别不是特别大，没有必要为这个纠结。学习爬虫开发的套路，能用Python2实现，换成Python 3 不是事。欢迎看看python爬虫联想词视频和代码https://zhuanlan.zhihu.com/p/21430020跟黄哥学python爬虫抓取代理IP和验证。https://zhuanlan.zhihu.com/p/21648138跟黄…显示全部
作者：杨航锋 链接：python爬虫如何按序抓取一个页面上的图文？ - 杨航锋的回答 - 知乎 来源：知乎 著作权归作者所有，转载请联系作者获得授权。 搬运一个以前的回答！这样是可以做到的，起因我和几个同学打算做一个针对大四毕业生的网站，所以需要整合学校…显示全部
比特币国内刚兴起的时候，写了个爬虫自动爬比特币中国，根据价格和策略进行交易。赚了一笔钱吧。 后来挖矿去京东抢显卡，写了个刷京东的爬虫，爬回来之后运行其中的js加载页面，如果有货就给自己邮箱发邮件，点链接抢显卡。显示全部
python只是个工具。有了工具主要看思路。 这个问题很有意思。如果你想攻击一个餐馆你该怎么做哪？这个问题可能听起来可能有点有问题，不过其实好多攻击网站的方法和攻击餐馆的套路也差不多。（写这个不知各位看官反应如何，反正我觉得挺有意思的，哈哈。） …显示全部
谢邀。 我个人写爬虫抓取网页的经验并不多，在我有限的经验里，基本上没有用过正则来解析网页，原因有二： 1. 正则不好驾驭 2. 有其他更方便的选择，如 XPath 或 CSS selector XPath 可以用 lxml - Processing XML and HTML with Python，很强大的 XML 和 H…显示全部
你的xpath写的太长了吧，很容易出错的 内容的tr的class都是provincetr，用这个筛选容易多了，//tr[@class="provincetr"]/td/a/text() xpath不熟悉，看了下wiki写出来的，写的不好 补充，这个才算真正回答你的问题，为什么浏览器可以，但是lxml不可以 因为浏…显示全部
正常来说，500不是你的锅。（参考阅读：HTTP状态码详解）然而对爬虫来说未必。请求参数不对，该给的没给或者给错了，又碰上对方程序员代码也得不够优雅，就扔给了你一个500。这时你应该用浏览器工具看下正常的请求带了什么参数，你也照着做。你被发现是爬虫…显示全部
是在广告自己的爬虫遵守协议吗？显示全部
咦…… 这不是说我们最近在做的项目嘛。 我们16年以来发现一个问题，首先爬虫的需求越来越多，同时爬虫需要的代码基础造成的专业性门槛让我们觉得，是时候节省大家的时间集中在数据本身上了。 {悄悄说一句：欢迎来造数：造数 - 最好用的云爬虫工具 关于使用…显示全部
你最在意查询速度了。 那你知不知道有个东西叫做倒排索引？这个查询速度比正排快的不是一星半点。 不知道的话，就去百度。 然后写个程序，把所有的倒排索引放到内存里面。然后内存放不下所有倒排索引，该如何存放？不过你数据量小，应该不是问题。 当然，如…显示全部
关于python爬虫，无论百度搜索结果还是知乎里的答案都已经很多了，前人之述备矣。 我自己有一点建议，python爬虫重点不是python，而是网络爬虫。学习爬虫你会把大量的精力放在网络技术和网页制作上，什么xpath啊css啊url啊，这些东西都不是纯粹的python知识…显示全部
我做过一个微博用户相关的的，还用face++进行了过滤。总共做了858万条数据，后来懒，没继续下去，其实也可以根据城市、年龄、兴趣标签等进行筛选。 妹子版： http://42.96.156.89/meizi/ 帅哥版： http://42.96.156.89/sg/ 一页16条，可以随机更换显示全部
前提肯定是你在浏览器里有权限看到这些数据。 看你的意思是作为商户端要自己的销售数据，那就真的别麻烦写爬虫了，淘宝现有的API完全能满足你的要求，去淘宝开放平台看API文档。如果你的商户规模不大，还不到淘宝对自有系统接入的要求，那就去应用商城买第…显示全部
前面都是本bug生产者的胡言乱语，我的脚本和gui在文章末有链接。目前功能有每日排行，关键字，画师作品，画师收藏夹，均支持多线程，仅每日排行有协程。（-a 开启）。还有数据库方法代替关键字，还在测试中。（数据库倒是已经爬完了）――――――――――…显示全部
我觉得问这种问题的人没什么学习的毅力显示全部
目前这些回答都是些啥… nohup & 或者 supervisor显示全部
“网络很好但是有好多页面都无法打开”你这多半是把人家网站爬爆了吧。一般不限爬虫的网站有两种，一种是公共信息型的非盈利网站。这种出于数据公开、开源等原因基本不限爬虫。另一种就是小站，别说限制爬虫，访问人多了都能宕机。 不管对哪种，PO主行为都…显示全部
[流量预警，6张图] 初步分析了下,应该是用了自定义字体。以前没怎么见过，只听别人说过， 很好奇，就分析了以下，还是很好破的，请看我的分析过程：首先，看网页源码： 这种形式应该是字符的某种编码，和字体文件是一一对应的，分析css后发现他用了myFont这…显示全部
题主想做的这个东西，抽象到上一层，是社会媒体（微博）的热点话题预测。想做和食品安全相关的东西，建议看看复旦大学吴恒做的一个叫“掷出窗外”的网站，该项目获得了上海市政府官方的支持。可以借鉴一下，有个大致的方向。爬虫软件是很好写的，我本人也写…显示全部
造数 是我们最认真的作品，最近我们一直在后台看到每天有一定的 pv是用户问“造数怎么用？” 现在果然知乎也有了这个问题。 【造数  是一个什么网站】 先说说我们搭建这个网站的目的吧： 我们做 造数  ，目的是 从数据采集的层面帮助大家完成三种需求，分别…显示全部
泻邀。 90%的用法是拦截HTTP请求，分析request和response的具体细节，协助排查前后端联调开发中的问题。 9%的用法是打HTTP“断点”，修改request请求，绕过前端js限制。这个不限于本地还是remote服务器。因此，对于高手来说，前端js的限制基本不算限制。黑…显示全部
重要的不是你学的怎么样而是你用这门语言做成了什么事显示全部
题主报坐标，东风洗地！显示全部
你的form data不对吧？三个参数method,params,_xsrf，其中params是个dict 你的代码是直接给了一个dict，method成为了这个dict中一项，另外也没有提供xsrf，会被认为是跨域请求 我去试了下，觉得500的原因应该是params的格式问题，你提交的内容没有符合标准…显示全部
这是一个集成的Python环境，一键安装，装好即用，适合懒人，适合初学者。特别的，对Windows环境而言，因已附带很多第三方库，就不用为了安装这些库，而安装C++编译器了。 其使用conda来管理包，集成了Python主程序，IDE（Spyder）与IPython，以及常用的第三…显示全部
100+ Interesting Data Sets for Statistics - rs.io 拿去，也许能给你的数据分析项目提供点思路。Hadoop 只是一个工具，数据分析重要的是灵感和方法。显示全部
其实用C#就完全可以满足，我就不贴代码的，手机码字，就提供一个思路，应该不用100行代码，步骤如下： 1，使用webclient或者webrequest这两个类先访问版块 2，这个爬下来后用正则表达式获取所有的贴子链接，不懂正则百度 3，分就更简单，看帖子列表…显示全部
可以的 这里有个曲折的故事。 某场tc前，我打算去打一下（很突然，因为不太打tc），然后开始配置客户端嘛，就是几个插件，其中有个让我填存生成的html和cpp的位置，我填了~/code/tc，然后测试发现插件帮我在我的~目录下新建了一个~,oh，愚蠢，我就打算删掉…显示全部
有啊，只是没说出来而已。 我们就在爬扣扣空间 因为学校要跑早操，但是早操信息是由体育部每天发在扣扣空间的，我们就每天六点到七点，每隔三分钟爬一次它的说说，然后分析下今天是否跑操。 另外，我觉得爬扣扣空间最麻烦的就是加密，腾讯的加密js都写了三…显示全部
Html Agility Pack：感觉比较好的东东。 http://htmlagilitypack.codeplex.com/ Arachnode：暂无说明 http://arachnode.net/ NCrawler：暂无说明 http://ncrawler.codeplex.com/ HttpHelper：国人的，貌似是个小团队，专门写写帮助类框架，有开源的，也有功…显示全部
靠 不请自来，今天刚做了一个妹子图爬虫，如何防止重复爬取，我的思想就是每爬取一个网页，就将其插入数据库中，如果程序停止，就用需要爬的网址列表的集合减去已经爬到的网页集合，得到的结果就是没有爬过的网址。# 需要下载的地址 这里采用mongodb数据库…显示全部
说来忏愧，刚提出这个问题就自己找到了答案。 我用节点的.string属性得到None是因为：如果tag只有一个 NavigableString 类型子节点,那么这个tag可以使用 .string 得到子节点，如果有两个就得到None。而这里的div节点有两个子节点，一个是文字内容，一个是注…显示全部
抓取网页时，之前我也经常遇到编码问题，后来不断摸索，已经找到所有这类问题的解决方法了。首先，linux默认是utf编码，windows默认是gbk编码。然后网页也有编码方式。Unicode作为中间编码，首先把网页编码解码为Unicode，然后编码为你的系统编码.。问题解…显示全部
myPage 已经是utf-8 就不需要再decode() 看python培训黄哥的三篇文章 python开发爬虫汉字编码不再是问题： 将python2中汉字会出现乱码的事一次性说清楚 article/python_bianma.md at master ・ pythonpeixun/article ・ GitHub python爬虫访问多个网站、中…显示全部
https://qqqun.org 自己去查吧显示全部
方法多种多样。核心思路是，在本次爬取之后到下一次重新爬取之前，处理好上一次已经爬的状态。或者说，做好日志（log）。不仅仅在爬虫处理断点时，任何需要跑很久并且都可能需要重启的程序都同理。我在实际开发中常用的办法有：1. 写一个函数，在每次爬取的…显示全部
谢邀，我好像有一点点资格回答这个问题，各位请看我的签名。 之前学Python时写了一个爬虫，爬取知乎的问题数据，分析比较后选出500个最可能火的问题放在那个网站上。 其中的爬虫已经开源了，传送门：MorganZhang100/zhihu-spider ・ GitHub 各位用Github的…显示全部
题主握手 我也一直在做对亚马逊的爬虫，也饱受这个反爬虫的困扰。 在经过一段时间的爬虫之后，亚马逊会返回一个RobotCheck的页面。但是如果这个时候暂停爬虫，快的话个把小时，慢的话半天，基本就又可以继续爬了。 所以判断的规则无非是根据IP，访问行为来…显示全部
网页上有JavaScript，最直接的方法是运行这些JavaScript（还有传统方法：抓包。现在操作起来比较麻烦，因为很多异步AJAX网站有太多包了），能解释运行JavaScript，那就是浏览器的行为，虽然有些可以做的很简单，但是，仍然可以看成一个小浏览器。接下来又分…显示全部
这个问题已经有人做出现成产品了，你可以看一下: http://sleepingspider.com 注册成为用户后，可以选择需要关注的网页，如有更新会收到邮件提醒。还有一些高级的设置，没用过，你可以看看显示全部
为了保护数据，网站会不时修改算法，下面的经验是过去没多久我给一高校的大数据实验室做的方案。 爬搜索指数，要解决3大技术问题：1）模拟鼠标在图表上移动，让爬虫抓取悬浮出来的信息，要使用动态爬虫技术。2）数字是图片格式的，而且不是完整的图片，要拼…显示全部
以下图电影为例： 获取： python3代码：import reimport requestsRegEx = "<h3><a href=(.*?) class"url = 'http://www.zimuzu.tv/resource/index_json/rid/25610/channel/movie'header = { 'Host': 'xiazai002.com', 'User-Agent': 'Mozilla/5.0 (…显示全部
看文档和例子代码，是高效学习库的正确方法。欢迎看看Python 爬虫需要学习的知识点python爬虫联想词视频和代码Python爬虫联想词视频和代码 跟黄哥学python爬虫抓取代理IP和验证。跟黄哥学Python爬虫抓取代理IP和验证。跟黄哥学python爬虫抓取代理IP跟黄哥学…显示全部
久了之后就会被对方网站监测到你的大量长时间爬取的行为，就会有对应的防采集措施，就是你看到的超时等问题，其实你的网络是好的，只是对方网站拒绝给你正常数据，要继续采集有几种办法： 1. 降低采集速度，等待一段时间之后，封锁解除再来采集，一般都没问…显示全部
import requestsfrom bs4 import BeautifulSoupr = requests.get("http://club.mil.news.sina.com.cn/thread-666013-1-1.html")r.encoding = r.apparent_encodingsoup = BeautifulSoup(r.text)result = soup.find(attrs={"class": "cont f14"})print…显示全部
为啥会有人邀请我回答←_← 首先协程算是用户态的线程，优势主要是少了内核态用户态的切换和能自己来做调度。 然后协程一般只在有IO操作的时候才能用到，对于一些会阻塞的IO操作，可以自己选择协程切换，等IO就绪了再切回来，可以更充分利用CPU。 就像你在…显示全部
一般的搜索引擎（比如谷歌）都会遵守规则，当然不排除有不要脸行为（比如百度）。 维基百科： Robots.txt协议并不是一个规范，而只是约定俗成的，所以并不能保证网站的隐私。可以遵守，也可以不遵守。显示全部
不是必应不遵守，是 robots.txt 写错了。 另外，反对 @李哲 的说法。 robots.txt 协议不是法律法规，也不是行业规范，这没错。但是一个搜索引擎声称自己遵守 robots.txt 协议那就有道德责任遵守。 说遵守不遵守无所谓是不对的，哪怕是百度这样无底线的搜索…显示全部
谢邀。 Python有captcha验证码识别库，可以应对简单的识别任务。 输入是图片，输出是识别文本。 https://pypi.python.org/pypi/captcha-solver/0.0.3 遇到更复杂的，可能得用Tesseract OCR 引擎 http://soaptek.blogspot.com/2012/11/bypass-captcha-using-python-and.html 还解决不了的话，一般是因为验证码太复杂，可能得自己训练神经网络来…显示全部
谢邀，本人也是刚开始学习爬虫，中间可以说是走了不少的弯路，这里说下自己的经验，希望能有所帮助。 ****************************************************** 开始学爬虫的时候我也是查了各种资料，到知乎上把爬虫的相关问题都看了一遍，大家都在说Python…显示全部
requests,re,threading,Queue,os有这几个包基本上就够用了。当初学习爬虫的时候一点都不懂，甚至连爬虫是什么都不知道就在学了，但是怀着不懂装懂的精神，到现在基本上也算对爬虫了解一二。 正如你所说，爬虫是个大坑！因为这不仅仅是Python的事，想要学好…显示全部
不是静态网页的数据,用这个当然搜不到. 用着几个数据去post这个网址,才能得到你要的数据,如下,以json格式返回 显示全部
也许是针对豌豆荚的一个应用：豌豆荚一览吧，这货会抓取文章…显示全部
我看你的意思是像要做爬虫爬这个网站的数据，但是返回403是吧。那就按照我这几年的爬虫经验，来跟各位探讨一下。 首先，你是在模拟web请求，而不是在调用api，所以，我反对楼上说的ip限制的可能性。 第二，网站有可能会根据你的行为判断，但是那是发生在你…显示全部
你可以根据日志查一下，看看程序再抓哪些页面的时候死了，再分析一下为什么死。有些网站为了防止爬虫会采取一些措施，比如给你设计一个链接，你打开这个链接后跳到另一个页面，这个页面上还有一个链接，又链接到原来的页面，如果处理不好爬虫就在这死了。还…显示全部
我们一直使用思路二中的方法1，也就是说用一个浏览器内容来运行JavaScript和解析动态内容，而用python模拟人在浏览器上做动作。 这个实现方案最自然，虽然有人一直在批评这样的速度比较慢，但是在实际运行环境中，大部分情况下你会担心运行得太快了，我们采…显示全部
Python2配windows简直婊子配狗，遇到编码问题我都是抱着吃屎的心态去解决的。。。。你这全都是unicode还没啥，打印不出来估计是因为你用的dos窗口，默认编码是gbk。如果出现了看不懂的编码格式以及奇怪的符号，推荐beautifulsoup库的damn。。。应该是这么叫…显示全部
应该是这个网页head文件里面没有编码， 需要自己指定‘UTF-8’才行。 我只会requests这个库自己指定编码的方法。 显示全部
取决于你要怎样查询,比如按标题关键字查询?单关键字模糊查询的话,用mysql应该就可以.(mongodb没有多少区别).更复杂的情况的话最好用 elasticsearch. redis,leveldb没有提供查询的条件的.需要自己处理索引问题.显示全部
谢谢邀请 看了一下你说的课程，讲的是基本语法，无从下手是正常的，所以先不着急。 学习一门编程语言（或者其他技能）的目的是为了解决我遇到的问题； 而不是，先学了，再看他能做什么； 简单说就是带着目的去学习。 比如，你想做一个网站，当你什么都不会…显示全部
爬虫的入门是很快的。 了解HTTP协议，了解几个API，就可以直接上手写代码了。 推荐：《自己动手写网络爬虫》一书，里面详细讲解了爬虫的基本原理和实现，尽管API有些过时了，但是思路是很棒的。 爬虫的学习分为下面几个阶段吧： 第一阶段：了解HTTP协议，入…显示全部
爬不爬的都无所谓了，你需要的只是一个好用的html解析库而已。个人推荐jsoup。显示全部
爬电商网站数据，要特别注意控制速度和间隔时间，因为他们的反爬虫监控是最严格的，如果爬得太快和太频繁，就很容易被发现，结果就是爬虫无法访问网页了。 我也爬过亚马逊评论，不过我用的是集搜客网络爬虫，可能跟火车头的爬虫思路有点差异，给你介绍一下…显示全部
Deep Web 深网 详细介绍 https://pao-pao.net/article/230 请翻墙访问显示全部
推荐使用东方财富网抓数据，因为可以直接保存为excel文档，后期处理也相对方便，思路如下： 1.先得到需要的上市公司的股票代码和名字。这一步可以参考 @段晓晨的答案！ 2.分析下载链接地址。以康达尔为例，年报地址http://soft-f9.eastmoney.com/soft/gp14.…00004802显示全部
可用队列实现这个。和迅雷的断点续传不是一回事。 用一个双端队列，存贮采集的url，采集的从队列pop出，有新的的添加。 不想采集的时候，讲还没有完成的写到文件中，下次工作时，再读到队列中。显示全部
答主的第一次就就交在这里了，，， ――――――――――――――――――――――――――――――――――――――― 前不久学习了python，正好复习一下 代码如下： import re,urllib page=urllib.urlopen('http://m.acg.tv/video/av2046040.html') HTML=page.read() re_t…显示全部
python2乱码可以这样解决。 能够实现用print函数打印的编码是正常的，同时保存为文件的也正常，需要弃用requests而用urllib2源代码如下import urllib2 import sys content=urllib2.urlopen(“http://……”).read() typeEncode = sys.getfilesystemencoding…显示全部
主要列出来Java和Python类的爬虫框架对比： 如果你想只是做简单的页面采集，数据量小或是一次性抓取，建议选用上手容易的框架开发。 如果你想构建一个垂直数据采集平台，建议使用功能丰富、支持分布式的框架进行二次开发即可，正所谓站在巨人的肩膀上。显示全部
推荐你用一软爬虫软件，GooSeeker，http://www.gooseeker.com 1.比火车头好用，火车头好贵。 2.如果你要的数据量特别大的话，可以在社区上寻求帮助 3.其实你的要求真的很容易实现。。显示全部
我是单纯来看吐槽的，可是看到大家的反馈，自己也实在憋不住了，一吐为快吧。 记得是大三的时候吧，装上了头条的客户端，当时最让我眼前一一亮的是他们产品广告语：您所关注的，就是头条。 文章品质在一定的时间内还是不错的，我当时比较关注的就是里面的当…显示全部
根据 OAuth2 协议，Authorization 头里的数据是你的 Token，用来确认你的身份，你自己并不能生成，是服务器下发给你的。Bearer token 是一种 token 类型，RFC 6750 定义了 Bearer token 的用法，题主可以参考：「The OAuth 2.0 Authorization Framework: Be…显示全部
由于本学期好多神都选了Cisco网络课, 而我这等弱渣没选, 去蹭了一节发现讲的内容虽然我不懂但是还是无爱. 我想既然都本科就出来工作还是按照自己爱好来点技能吧, 于是我就不去了. 一个人在宿舍没有点计划好的事情做就会很容易虚度, 正好这个学期主打网络与…显示全部
这个要创建一个ip代理池，定时检测，保证能用的匿名代理。1、可以从代理网站上抓取2、也可以自己扫描3、也可以购买。4、购买云主机，可以有一些ip。5、可以定期断网，更改ip。.........................显示全部
不追求大量采集的话，用模拟浏览器就可以了，一天几十万很轻松。运用phantomjs无头浏览器破解四种反爬虫技术 - 知乎专栏要是嫌phantomjs难用的话，可以去学一下selenium同样能处理。。显示全部
最高回答，不要以为现在ai热，就扯ai来吓唬人，大概率是通过风控平台上的一堆规则来判断的，人工积累的一些规则。采集鼠标运动轨迹。浏览器信息，等等之类的。阿里网站常用的两个js，nc.js收集鼠标轨迹和um.js生成设备指纹。显示全部
只不过是表示方式不一样而已，程序用起来没差别。json.dumps 的时候加参数 ensure_ascii=False 就可以看到汉字了。你需要 JSONView 这种浏览器扩展来漂亮地展示 JSON 数据。显示全部
现在的网站越来越动态化，越来越像一个会话系统，不建议通过模拟http消息的方式抓取网页，用这种方式能抓取的范围会越来越少。在一个会话系统中，http消息头里面有很多参数是很快失效的。你辛辛苦苦抓包模拟了一个消息，10分钟后可能就失效了，有些失效时间…显示全部
根据题主的情况，我可以推荐一些书 入门： 1《开发自己的搜索引擎 Lucene+Heritrix》：根据这本书，你就可以搭建简单的爬虫了，已经可以完成你的爬取需求。 2《这就是搜索引擎：核心技术详解》：该书你可以了解爬虫的基本算法，对爬虫的机制有一定了解。 3…显示全部
//dd[contains(strong/text(), '建筑类别')]/text() contains()函数是最常用的，根据文本中的标志文字定位节点显示全部
谢邀！ 无论采用什么混淆技术，最后都要在浏览器中把数字显示出来，所以，如果采用抓包，然后模拟http消息的方式抓数据，题主说的问题都存在，挨个破解确实挺麻烦的，但是，如果直接从DOM中抓数据，题主说的那些问题都不是障碍。我写的GooSeeker爬虫是直接…显示全部
这个问题大概算解决了吧 可能是京东对同一个商品的访问有一定限制 比较随意的方法就是不要按照商品页数爬取 打乱商品链接的顺序 如果能做到短时间访问的都是不同的商品 同种商品的时间间隔开 基本就不会重复或者返回空白了 嗯。。。 不过京东真的是有非常多…显示全部
过滤多余标签：string(.).xpath('//a')[0].xpath('string(.)').replace('\n','').replace(' ','')显示全部
谢邀 首先纠正题主，Python用的是解释器，不是编译器。 我使用题主的代码后跟题主出现一样的问题，原因和解决方法如下 BeautifulSoup用法不对 BeautifulSoup() 第一个位置参数的类型应该是字符串类型或文本文件句柄，第二个位置参数是可选的，用来选择Beaut…显示全部
不要用C++ 不要用C++ 不要用C++ 如果你是以爬取数据为目的，而不是以学习C++以及多线程编程为目的，强烈不建议C++ 如果是为大公司开发大型数据爬取系统，另当别论 小公司或者个人开发爬虫，或者要求开发周期短，强烈建议Python 没写过爬虫，熟悉一下，很快…显示全部
启动python程序或python解释器前，先设置代理地址 在windows下应该是这样的: c:\> set HTTP_PROXY http://地址:端口# 或者有用户名密码的情况c:\> set http_proxy=http://username:password@地址:端口# 然后再启动 python程序或python解释器c:\> pyth…显示全部
关于协议书上说得太多就不解释了，我觉得在安装系统的时候就已经安装了大部分常用协议，但是事实上还有少数协议需要额外配置甚至是安装。这里举一个例子对比一下，打开电脑浏览器就能正常查看网页，因为预先配置好的TCP/IP协议簇和HTTP协议正在发挥作用，但…显示全部
你可以用AioHttp，它把HTTP封装好了。 当然你也可以自己依靠asyncio的create connection写一个框架。 我这里建议使用Python 3.5。你可以用await和async def 区分协程和生成器。 言归正传， 首先，你应该谨慎使用协程，只有在涉及到IO的时候，使用才是合理的…显示全部
蟹腰 先上代码： # -*- coding: utf-8 -*-import requestsimport reimport datetimeimport osimport csvclass Spider(): def __init__(self): self.url=u'http://www.nea.gov.sg/anti-pollution-radiation-protection/air-pollution-control/psi/hi…显示全部
https://github.com/maoweixy/Web-Crawler 这是我写的，可以实现验证码登陆 里面有详细的代码说明 看完了记得给赞哟~显示全部
因为最新的OSX有SIP机制，安装的时候命令加上user pip install --user 命令 即可！显示全部
八爪鱼是工具，python是代码，八爪鱼的目标是让有需要采集网页的人都可以使用工具轻松达到目的，就这个目的来讲，八爪鱼就是要取代众多公司自己爬虫工程师团队开发的python爬虫程序，我觉得完全取代有点困难，总有些人就是一定要求自己开发的，这种就没办法…显示全部
我用的Python3, 直接贴代码, 暂时只会怎么自定义UA, 见笑了, 不懂的欢迎追问~ from selenium import webdriverfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilitiesdcap = dict(DesiredCapabilities.PHANTOMJS)dcap["phant…显示全部
参考：机票数据采集系统策划方案介绍 机票数据是一种实时性要求很高的网络数据，同时机票采集数据的用途也很多，数据采集的量也会相应增加。机票数据采集系统的核心目的是通过采集［去哪儿］数据源的机票实时价格信息，动态分析出需要的价格内容，并上传到…显示全部
@王川 的回答是使用了api的方法，我说一下使用爬虫的思路。 我以前爬取了各大社交网站的数据，如人人，新浪，腾讯。 我当时需要爬取了所有大V的信息。我首先pass了api的方案，因为api的数量限制，不可能让我大规模的抓取数据。 第一种是直接get json信息。…显示全部
搜狗采集时用了openid,然后通过js SogouEncrypt.encryptquery 得到一个加密后的字符串eqs = http:// http://weixin.sogou.com/ gzhjs?cb=sogou.weixin.gzhcb& openid=oIWsFt_h4EqEMRKg4H13hAZvL4tE&eqs=3QsKosugOrckotzP8zXJgutooB6cxntDqjNG81%2BF5fIhCUca9KZL…显示全部
你可能会把 NodeJS 用作网络服务器，但你知道它还可以用来做爬虫吗？ 本教程中会介绍如何爬取静态网页――还有那些烦人的动态网页――使用 NodeJS 和几个有帮助的 NPM 模块。网络爬虫的一点知识网络爬虫在网络编程世界中总是被鄙视――说的也很有道理。在现…显示全部
听上去有点像前两天爬取回答的点赞用户列表时遇到的json数据…… 你可以先看一下这里如何入门 Python 爬虫？ - 段晓晨的回答 不过那个不用POST是这样的。 _xsrf=f6cf0e800c0d33fdf8c8c4f3d0701f4amethod=nextparams={"offset":20,"order_by":"created","h…显示全部
因为京东的评论内容是需要一次点击事件才能加载的 a_list = browser.find_elements_by_xpath('//a')for a in a_list: if '商品评价' in a.text: a.click() 找到评论的按钮，然后触发一次 click 事件，等待加载结束，最后抓取内容。显示全部
目前微信公众号文章内容的大批量采集已经得到完美解决方案！ 日期2016-12-25 解决方案在这里：知乎专栏 和楼上所有方法截然不同的思路，从微信客户端采集，注意是采集而不是爬取。 方案特点： 1、自动批量公众号大规模采集 2、采集结果可再次利用 3、方案可…显示全部
刚刚好，我前不久刚学Python 爬的就是我关注的人的信息，写成了博客，全过程都在这: http://m.blog.csdn.net/article/details?id=52159871 我想你拿到403的原因是因为Post的data 里面没有加入_xsrf的缘故，这是用来防伪登陆的，虽然你模拟登陆知乎的时候传…显示全部
估计服务器判了一下referrer。显示全部
你的输出是在每个线程里各有一段？ 因为关于输出的问题，大都不太可能做到原子化，而这样需要输出/r的进度条就更是了。 解决方案是，只有一个单独的线程管你的输出，其它线程通过通信告知进度更新而不直接输出。 同样的思想，也出现在各种MVC中。显示全部
你只抓一个行业的话没必要用crawler，很多投研都是靠公开的资讯网站做初步信息收集的。投资潮 金融界 crunchbase 扫一遍VR方面的投资信息应该就有数了。 当然，想深入了解这个行业还是要多找业内人士请客吃饭聊天喝茶呀。显示全部
这就是为啥现在大家都拿Webkit改成爬虫啊。去搜crawler based on webkit有一大堆。要对付的问题无非就是这么几种： 1）后期行为生成/消除的内容 2）人机读取页面的行为差异 题主的例子，只要在DOM中找到这个链接元素然后去找它对应渲染的页面上是否存在就行…显示全部
楼主可以看一下cola分布式爬虫 Cola：一个分布式爬虫框架 Cola首页、文档和下载 另外，还有scrapy，这个可以参考豆瓣小组的使用scrapy,redis,mongodb实现的一个分布式网络爬虫 这方面内容其实还是挺多的，多去找，最重要的是去练系，动手写显示全部
teleport pro Teleport Pro Teleport Pro -- Offline Browsing Webspider显示全部
跑了下你的程序，没有获取到任何内容，首先“糗事”没获取到，其次url就算获取到了的话，是“/text/page/2”你没经过处理连接起来之后是https://www.qiushibaike.com/text//text/page/2奉上自己的代码# Python3.5import reimport codecsimport requests…显示全部
在百度图片搜索minions(随便什么) 打开Chrome console,选择Network XHR 然后下拉加载 点击进去，看下Request URL 还有response, 发现是包含图片数据的json 然后，直接get那个Request URL借能得到图片的json数据了 再然后，解析json得到图片url, 下载图片时…显示全部
1、先学标准库，搞懂原理2、以后用的时候，尽量用requests显示全部
谢邀！ 既然专家们都答能，那我就答不能吧！ 比如说，《大数据时代》这本书里很多案例已经被证伪！ 未来代表什么？一半是人性，一半是天意。 人心难测，这句话不会随着科技的进步而改变！ 就算是华尔街精英们，股市的动荡往往也会被某个突发的、名词相关的…显示全部
503是服务器错误，除非对方发现你是爬虫了，故意返回错误代码（可能性比较小），否则跟你爬虫关系不大。解决办法： （1）你可以设置重试机制，即当某一次获取失败之后，重启将url放入待抓取队列。 （2）不建议使用urllib2，建议换为requests。 （3）建议使…显示全部
可以 app里的数据比web端更容易抓取，反爬虫也没拿么强，大部分也都是http/https协议，返回的数据类型大多数为json。 通常通过抓包工具来查看请求的接口和参数，简单的可以用fidder，手机端设置代理即可，如果要抓取像声音/视频这种udp的包 可以使用wiresha…显示全部
因为这个世界上有个东西叫做js，有项技术叫做ajax，所以目前的大多数网页，我们看到的效果并不是html本身的内容，而是在此基础之上执行js后修改dom重新填充得来的内容。显示全部
如果只是模拟一个get请求还是挺容易的。import socketdata = b'GET /get HTTP/1.1\r\nHost: httpbin.org\r\nConnection: keep-alive\r\nAccept-Encoding: gzip, deflate\r\nAccept: */*\r\nUser-Agent: python-socket\r\n\r\n'url = 'httpbin.org's = …显示全部
Network里面勾选preserve log显示全部
打开 Chrome 开发者工具，然后看一下那个 api 接口，然后照着参数发请求就好了。 显示全部
这是我学习Python爬虫时写的一些代码，都比较简单，适合初学者！https://github.com/Fenghuapiao/ 题主有兴趣的话可以fork下来跑跑显示全部
好像一不小心编辑的时候把一开始的话给删了…… python是可以自己管理cookie的 相比而言比直接到header里面写cookie方便许多 我手头不是很方便也没法测试这两种方法的区别 给一条明路就是可以用wireshark等工具抓包看看两种方式的数据包有什么区别 顺便贴一…显示全部
visual scraper，不用写代码也能通过图形界面快速定义出一个爬虫来用，比如Portia。显示全部
sorry，好久没上来了，刚看到，根据我的了解是：搜索引擎大致可分为四个子系统：下载系统、分析系统、索引系统、查询系统。而爬虫只是下载系统，至于为什么叫做爬虫，我自己理解是形象类比吧 显示全部
看这两张图应该就清楚了 显示全部
谢邀，我不是很了解爬虫的实现以及相关技术，只略懂皮毛，根据我知道的瞎猜一下。 限制爬虫，我想到三种办法： 第一种是通过robot.txt，这个是种君子约定，你手动忽略它也不会有人给你判刑。 第二种是根据UserAgent，这个可以欺骗，你换了个Chrome浏览器的U…显示全部
#!/usr/bin/env python# -*- coding: utf-8 -*-# @Date : 2015-11-10 16:21:59import requestsfrom bs4 import BeautifulSoupurl = 'http://www.nea.gov.sg/anti-pollution-radiation-protection/air-pollution-control/psi/historical-psi-readings/y…显示全部
你提交的数据写错了 以下代码登陆成功: #!/usr/bin/env python# -*- coding: utf-8 -*-# @Author: LostInNight# @Date: 2015-10-28 19:59:24# @Last Modified by: LostInNight# @Last Modified time: 2015-10-29 01:11:22import requestslogin_url…显示全部
找得到人，找不到朋友…… 最要紧的是，你得多年前有朋友可失散……显示全部
Tushare显示全部
谢邀。 有句传言，不知真假。“用Jumony就是为了把cnblogs爬宕机” 爬网站首选Jumony，虽然作者本意是拿来做ViewEngine的，这就是人生啊~啊哈~！ 内啥，钱发我微信就好了 @Ivony显示全部
因为其中'动态网'三个字是敏感词, 类似于那个笑话: '一台独立主机'中的'台独'显示全部
轮子和车子的区别。 前者要依附于一个程序，后者自己就能跑。显示全部
你提的这个现象，我在我同学的blog上经常见到，他是搞Linux内核编程的，目前在红帽工作。 我觉得有以下这么几种可能： 1，有的地方不允许留邮箱地址，因此把@换成#，以躲过系统检测。 2，在共公地方留下邮箱地址，很容易被各种机器人、蜘蛛爬到，要知道，垃…显示全部
我在简历写了爬过一个成人网站的所有小黄图, 面试的时候就是随便聊了聊, 没有问任何技术方面的问题就进去了.显示全部
看你使用场景。 如果你的爬虫是玩玩，练练手。或者是对某一站点请求并发量不大的时候，可以用scrapy。 如果你的爬虫对某一站点请求很频繁，量很大时，我倾向于使用requests bs re。 爬虫的业务逻辑很简单。重点是反爬！反爬！反爬！ scrapy优势在于抽象了业…显示全部
这个网提供了REST API可以返回国服的数据 DaiWan|带玩游戏平台 你可以试下显示全部
从谷歌创始人的论文《The Anatomy of a Large-Scale Hypertextual Web Search Engine 》里可以看到，谷歌一开始的爬虫是用Python写的。 http://infolab.stanford.edu/pub/papers/google.pdf 4.3 Crawling the Web In order to scale to hundreds of millions of web pages, Google has a …显示全部
我也贴一个一位大婶写的C#版百度空间爬虫工具：@sqybi 写了一个抓取百度空间文章和评论的软件 - 三千院大小姐的紫公馆 - 知乎专栏 要有耐心！爬完之后，得到了一个单一的.json文件，有图片（不过不知道是不是原始尺寸），有评论！（这个.json文件可以直接再…显示全部
先安装wheel 再下载 Twisted模块的.whl安装包Python Extension Packages for WindowsPython Extension Packages for Windows 再pip install scrapy 就能安装成功了显示全部
不请自答 知乎首答 作为一个刚刚实现了新浪微博爬虫程序的小白，我觉得我还是可以答一下的 我的爬虫最终效果是： 1，输入用户的id，将该用户的所有文字微博储存在一个.txt文件中； 2，将该用户在微博中发表的图片按照时间顺序存储在本地文件夹中。 我大致参…显示全部
此问题我也研究过，，1-2 用搜狗完成的，可考虑目前来说比较好！ 第三个问题，， 我试过虚拟机，，写截屏脚本，类似游戏外挂，，模拟点击，截屏然后orc 单个技术点都测试通过了，但是微信对虚拟机有免疫貌似，封停帐号了！！！ 其他方式还有，抓包找接口，…显示全部
既然你说chrome，只需要按一个键，还能自己换Device，我只能帮你到这儿了～ ============================================================ 2014.12.6更新 具体操作： 1. 打开chrome开发者工具 Mac：commond + option + i Windows：F12 2.左上角有个小手机…显示全部
用什么orc 都是标准的数字 扣出来算个md5或者floodfill一下就行了 http://www.baiduidx.com 随便搭了个服务 在线查询的 可以导出Excel 速度比较慢 有问题私信我 伸手党勿扰 vultr服务器太慢了。。没续费 暂时不提供服务了显示全部
使用冰点文库下载器。 具体介绍请看链接：冰点文库下载器。显示全部
网络互联是今后发展方向，而随之而来便是网络的保障问题，这一点从发生来的各种大小安全事件以及中美两国对网安做出的政策措施看出来，在20年网络攻防技术不会被淘汰，反而技术会日新月异，甚至开发出智能自动化网络安全产品，工具是技术的成果，技术是工具…显示全部
楼主，如果你自己不会做采集规则,或者想付费找别人采集数据，可以付费在八爪鱼官方数据及采集规则定制平台（数多多）发布定制，数多多平台有1000家左右的数据服务商像淘宝和天猫卖家一样，提供数据和采集规则定制服务。数多多平台服务商官方实名认证，平台…显示全部
用 Simple HTML DOM 这个PHP的DOM解析库:<?phprequire __DIR__ . '/simple_html_dom.php';$html = str_get_html('<img src="abc.jpg">');foreach ($html->find('img') as $ele) {echo $ele->src . "\n"; // 输出 abc.jpg}显示全部
判定为爬虫后返回虚假内容显示全部
题主 你好，我是神箭手的CEO，首先非常高兴你能关注到我们的服务。 关于更新的问题我需要解释一下，由于服务企业的数量的增加，我们早期的架构很难满足增长的用户的需求，所以我们决定在去年10月份左右对全套系统从底层进行重构，由于对整体工作量预估不足…显示全部
楼主别学bs了，网上文章很多都是过时的。建议使用 pyquery 或者lxml，前者是jquery用法，很简单，后者性能很高，避开了GIL。bs又难用，性能又差，楼主可以去对比一下。显示全部
Google了一下，并没有什么不同。我只能善意的推断，他们的Robots写错了。。。显示全部
抓包，再模拟post或get 电脑上安装抓包工具，手机通过代理上网，用下面的抓包工具抓包，再模拟post或get。 mitmproxy mitmproxy - home charles https://www.charlesproxy.com/显示全部
最快的话用echarts，现场模板，数据导进去；其他很多库都有热力图插件，python应该也有。 不怕麻烦用javascript从头写的话，就是画很多点，2个点靠的足够近的话就扩大半径，形成联接； 要设置透明度，混合模式设置为'lighter'显示全部
验证码没有库，基本上都是程序自动随机生成的。所以这种穷举的方法不现实。 可行的办法是进行验证码识别，常见的识别方法很多，比如神经网络，图像处理算法，模式匹配等等。 我曾今自己在这块写过一些文章，原理不复杂，你如果懂一些程序语言，你也可以按照…显示全部
能。理论上，普通人能访问到的网页，爬虫也都能抓取。所谓的爬虫抓取，也是类似于普通人浏览网页。与普通人上网方式不同，爬虫是一段自动执行的程序，能把访问到的网页记录保存下来。指定好种子（起始连接）、选择合适抓取策略（关键词搜索的结果列表、种子…显示全部
互联网采集工具的历史十分悠久，好像自从有互联网，就有采集工具，大型综合网络爬虫也是一类采集工具。我从事这个行业有10年了，技术需要时时更新，需要时时学习新知识。从大的方面说，html到html5、从静态网页到大量ajax内容，而且现在越来越多的网站不像…显示全部
数据库里显示全部
Pikachu 每次看到都会脑补一句： 去吧，皮卡丘 是不是感觉很可靠？显示全部
直接回答： https://www.zhihu.com/node/ProfileFolloweesListV2 详细回答： 如 @TweLveAndOne 所说的那样，你可以用 Chrome 浏览器的开发者工具看到所有的请求，其中就包括 Ajax 的目标链接及其请求报文。 比如查看题主所关注的人，先打开这个页面：…显示全部
http://import.io确实是一个不错的工具，公司是英国伦敦的，目前在美国，印度等地有分公司。 http://import.io作为网页数据采集工具，主要的功能都具备。但其最引人注目，大家觉得最好的功能叫做“”“Magic”，这个功能允许用户只输入一个网页，就…显示全部
………………显示全部
http://data.stats.gov.cn/tablequery.htm?code=AA020C 以这个为例, 用firefox的firebug找到这个就行. 显示全部
你写的例子不能http://doc.scrapy.org/en/latest/intro/overview.html?highlight=asynchronously: requests are scheduled and processed asynchronously. This means that Scrapy doesn’t need to wait for a request to be finished and processed, it can send another request or do other things in the meanti…显示全部
text取到的是decode之后的内容，你这样处理肯定是会有问题的。 你可以通过requests.get(url).content获取binary内容自己手工decode。 url='百度音乐-听到极致'content = requests.get(url).contenthtml = content.decode("utf8", "ignore")显示全部
说明淘宝认为购物党比价做的还不错，为了对得起体量千倍于购物党的阿里巨头抬爱，更需要把比价做到极致。 特求算法工程师加盟助阵（自然语言处理、图像检索方向），阿里的技术同学也欢迎。 显示全部
刚开始学BeautifulSoup，对着API撸的。 print语句里转码成GBK是为了避免CMD输出时抛ERROR。 #!/usr/bin/env python# -*- coding: utf-8 -*-"""抓取词根与解释http://etymonline.com/"""import requests, sysfrom bs4 import BeautifulSoupdef qu…显示全部
socket中添加一个超时 应该就能解决了： import socketsocket.setdefaulttimeout(timeout) 建议看一下gevent，可以应用到爬虫中，gevent可以将系统socke全部patch成非阻塞的 你可以同时维持N个连接，这样速度也会提高很多显示全部
有个大数据服务交易平台叫数多多，类似猪八戒平台一样，但是数多多是大数据垂直领域的平台，上面有很多用户真金白银悬赏找人帮忙采集数据，请看截图。我知道有些人在这里一个月赚上万的都不难。但是也有很多服务商竞争的，就是服务实力和服务态度了，多个人…显示全部
跑题一下，目前做着基础的埋点+日常监控产品数据工作，非常想要向着这个方向学习，最近杂七杂八研究了挺多。个人认为： 牛逼的数据挖掘师工作一定是建立在对业务逻辑非常熟悉的基础上的。毕竟数据只是工具，不是目的。 也就是说，至少要兼具【不傻逼的产品…显示全部
py3里的str类型都是utf8，你用的是windows，终端是gbk的，print的时候会将utf8解码输出。但是utf8覆盖全unicode集，而gbk只能覆盖部分unicode字符集，而页面里有不能编码为gbk（也就是gbk中不包含）的utf8字符，因此encode失败。 \u30fb如果没记错，应该是…显示全部
我觉得我必须回答你这个问题！ 因为我确实接触了一些公司。 贵阳和成都应该算是二三线城市了吧！ 过去我真不看好在二三线城市做大数据方面创业的事情。说下原因。 第一，当然是人才了，你是技术牛人你应该知道，大部分的技术型人才都在往沿海、往北上广跑吧…显示全部
别逗了 python能代替八爪鱼 因为无论你用八爪鱼采集什么，我都便宜一半用python给你采集 我认真的显示全部
网络爬虫的基本工作流程如下：1.首先选取一部分精心挑选的种子URL；2.将这些URL放入待抓取URL队列；3.从待抓取URL队列中取出待抓取在URL，解析DNS，并且得到主机的ip，并将URL对应的网页下载下来，存储进已下载网页库中。此外，将这些URL放进已抓取URL队列…显示全部
因为 python 写屏用的是 WriteConsoleOutputA，而不是 WriteConsoleOutputW显示全部
第一种方法，解压服务器发回来的压缩数据~ 第二种方法，去掉request header中的『Accept-Encoding』头去掉~ 两种方法选一个就好~ 究其原因，简单来讲就是，Server在与Browser通信的过程中，会按照Browser支持的压缩格式压缩后再回复，所以你在header中加入…显示全部
如果都用selenium驱动了webdriver，就不要去抓包分析了，也不用去尝试修改url了，直接控制webdriver做各种点击和浏览行为。这种方案用于突破爬虫程序识别方面也有些用处。 如果再像楼上说的用COM控制IE，那就更加进阶了。 还可以进一步，做浏览器扩展，因为…显示全部
我猜题主是看了我之前的回答以后自己去尝试那个网页的……吓得我赶紧去看了一眼自己之前贴的代码确保没有写错…… beautifulsoup要先soup = BeautifulSoup(page) 你最好先print page 检查一下页面里有没有内容，不然你在空里找到的自然是空。 或许你应该看…显示全部
mac的terminal是utf8字符，win的cmd是gbk字符，win正常的话，在mac里decode('gbk','ignore').encode('utf8')转化一下显示全部
用tushare，http://tushare.waditu.com 显示全部
第一个问题题主已经弄明白了，对于第二个问题有一篇大概是2003年的论文《To buy or not to buy: mining airfare data to minimize ticket purchase price》（它是《大数据时代》中的一个案例），这篇文章把机票价格预测的方法写的很详细了。显示全部
需要两个模块配合: 爬虫模块：单纯负责抓取和存储数据数据处理模块：处理爬虫存储的数据。如发现某个人某个持仓数据发生了变化，向你发出通知 该爬虫的简单的流程: 定时访问目标页面抓取当前目标页面的数据，存入数据库数据处理模块简单的流程： 定时访问数…显示全部
Update 2016-11-06 亲测最新可用方法 1. 爬取最新文章,老的方法是爬取 搜狗微信搜索(做好被坑的准备...) 2. 自动化批量爬虫, golang实现, 只需设置一个代理即可 GitHub - sundy-li/wechat_spider: 微信公众号爬虫 (只需设置代理, 一键可以爬取所有历史文章)…显示全部
要加个Cookie curl_setopt($curl,CURLOPT_HTTPHEADER,array('Cookie: ASP.NET_SessionId=xixpiynxc0d3yi55r4bves45')); 用IP的话加个Header就行了， $url = "http://210.38.207.15:169/web/searchresult.aspx?anywords=android&dt=ALL&cl=ALL&dp=20&sf=M_PU…显示全部
新手入门，写了一个很简单的，cookie先人工登陆网站抓包获取，然后就一直用那个cookie了，详见我的博客Python爬虫入门：下载Pixiv特定关键词的高赞作品 源代码python/pixivSpider.py at master ・ wjw12/python ・ GitHub显示全部
我没写过Perl爬虫但是可以随便扯两句吧。用AnyEvent的话实际上就是用异步IO。因为网页获取是IO-bound的，用LWP::UserAgent的话是最基本的同步IO方式，很多时间浪费在阻塞上了。那么常规的并发思路主要是两个改进方向，一个是用多CPU线程做并发，另一个是在…显示全部
只能亲自渲染一下然后看看这个链接是否可见了，现在大的搜索引擎都有这个功能显示全部
要抓取google play的用户评论，推荐使用腾讯wetest舆情监控工具官网：产品舆情 - WeTest腾讯质量开放平台该平台覆盖了目前主流的BBS论坛和应用商店。BBS论坛如：百度贴吧兴趣部落手游宝其他等应用商店如：苹果app storegoogle play应用宝百度手机助手小米…显示全部
Scrapy爬虫轻松抓取网站数据，以bbs为例详细介绍了抓取过程。显示全部
我爬了你说的那个网页，讲讲我遇到的问题: 1.你说的循环爬取其他页面，在其他项目中用循环一般可以搞定，可是你的这个，第一页和第二第三页的表格是不同的，所以要重新写规则，我懒，写了第一页后，就不想在写第二第三页了； 2.乱码问题，我用request爬取，…显示全部
免费->用的人多->拉来流量->但是不稳定->购买专业版->完成转化。如西刺右边的广告：免费代理IP_HTTP代理服务器IP_隐藏IP_QQ代理_国内外代理_西刺免费代理IP显示全部
方案一：模拟浏览器有一个屏幕截图可以指定你的截图区域，这个获得的图片是不会变得，方案二：如果你是用普通请求自己管理登录cookie来模拟的话，则不会自动触发图片请求，从源码中取得url再请求即可。显示全部
这个链接就是人数的,自己构造参数去post. 显示全部
使用Visual Studio Coded UI Test，操作IE上Google，直接滚他的结果（逃显示全部
爬虫选手推荐在电脑上安装一款抓包工具，比如fiddler2，比如charles.有什么用：1.分析网络请求，浏览器自带的调试工具与抓包工具各有优势。抓包工具的优势更专业，工具更多。同时有一些浏览器工具做不到的功能，比如可以查看python的网络请求，编码工具。用…显示全部
如果你只是想找个工具不想自己写的话…去用 youtube-dl 吧 一天N个更新能确保下载可用…想学习可以看源码 youtube-dl/youtube_dl/extractor/sohu.py at master ?? rg3/youtube-dl ?? GitHub EDIT: 刚发现youtube-dl里的sohu视频下载没法工作了…显示全部
你用代理，防爬就没办法追踪到源IP吗？只有真正的分布式才好解决滴。而且，你用代理的这些IP，是真的好IP嘛显示全部
url地址必须是http://或者 https:// 这样才行啊显示全部
其实B2C网站拒绝一淘爬虫是很明智的，之前马云用过，但是目前我来看是没有用的，一淘完全可以动用编辑来手动更新，呵呵，我相信没有网站可以打败这种方式的， 一淘的模式跟日本的一家购物比价网站相同，那家网站在日本很成功，基本日本人淘物都会先去查查。…显示全部
Chrome 开发者工具 Elements面板1. 右键审查元素打开的面板, 最简单的查找Ctrl+F<img src="https://pic4.zhimg.com/v2-5e9704e956a847be939b41e54b35d007_b.png" data-rawwidth="678" data-rawheight="248" class="content_image" width="678" data-original="https://pic4.zhimg.com/v2-5e9704e956a847be939b41e54b35d007_r.png"> 2. 使用CSS Selecto…显示全部
谢邀。抓包，找到关键URL： http://gs.amac.org.cn/amac-infodisc/api/pof/manager?rand=0.6111210449823676&page=0&size=20发现直接返回JSON，这类接口一般非常简单，不涉及JS，只需按照原来的HTTP报文重放即可。 至于此处，要点有四： 0. rand参数我没去…显示全部
谢邀。作为知乎首次被邀请。还是很高兴的。 你用的是request这个模块他的官方手册里是快速上手 ― Requests 2.10.0 文档 成为大神之路就是官方文档+google 然后其中你要保留的图片的二进制显示是这个r.content 我演示一下： 比如抓取https://www.urlteam.org/wp-content/uploads/2015/08/14110321028932-e1440584371928.jpg i…显示全部
用过的几种方法： 1. 正则匹配"下一页"对应的标签，取出其中的url； 2. 搜索下一页的url对应的标签； 3. 第二种方法加一个纪录，如果取出一排的最后一个URL发现是取过的就停止。显示全部
同意楼上，你要把header信息都加上去，完全模拟浏览器行为就可以抓 我现在用的是py3的urllib 以下为我使用的代码，挺简单就取出来了 import urllib.requestimport http.cookiejarimport recj = http.cookiejar.MozillaCookieJar()opener = urllib.req…显示全部
